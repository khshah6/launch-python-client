:py:mod:`launch`
================

.. py:module:: launch

.. autoapi-nested-parse::

   Moving an ML model from experiment to production requires significant engineering lift.
   Scale Launch provides ML engineers a simple Python interface for turning a local code snippet into a production service.
   A ML engineer simply needs to call a few functions from Scale's SDK, which quickly spins up a production-ready service.
   The service efficiently utilizes compute resources and automatically scales according to traffic.


   Central to Scale Launch are the notions of a `ModelBundle` and a `ModelEndpoint`.

   A `ModelBundle` consists of a trained model as well as the surrounding preprocessing and postprocessing code.
   Specifically, a `ModelBundle` consists of two Python objects, a `load_predict_fn`, and either a `model` or `load_model_fn`; such that


       load_predict_fn(model)


   or


       load_predict_fn(load_model_fn())


   returns a function `predict_fn` that takes in one argument representing model input,
   and outputs one argument representing model output.

   Typically, a `model` would be a Pytorch nn.Module or Tensorflow Keras model.

   .. image:: /../docs/images/model_bundle.png
       :width: 200px

   TODO should we include a specific example here?

   A `ModelEndpoint` is the compute layer that takes in a `ModelBundle`, and is able to carry out inference requests
   by using the `ModelBundle` to carry out predictions. The `ModelEndpoint` also knows infrastructure-level details,
   such as how many GPUs are needed, what type they are, how much memory, etc. The `ModelEndpoint` automatically handles
   infrastructure level details such as autoscaling and task queueing. There are two types of `ModelEndpoint`s:
   `SyncEndpoint`s and `AsyncEndpoint`s.

   .. image:: /../docs/images/model_endpoint.png
       :width: 400px

   A `SyncEndpoint` takes in requests and immediately returns the response in a blocking manner.
   The `SyncEndpoint` always consumes resources, and autoscales on the number of inflight requests.

   An `AsyncEndpoint` takes in requests and returns an asynchronous response token. The user can later query to monitor
   the status of the request. We may later expose a callback mechanism. Asynchronous endpoints can scale up from zero,
   which make them a cost effective choice for services that are not latency sensitive.
   Asynchronous endpoints autoscale on the number of inflight requests.

   In addition, we will expose another abstraction called a `BatchJob`, which takes in a `ModelBundle` and a list of inputs
   to predict on. Once a batch job completes, it cannot be restarted or accept additional requests.
   Launch maintains metadata about batch jobs for users to query, even after batch jobs are complete.

   Choosing between different types of inference:

   `SyncEndpoints` are good if:

   - You have strict latency requirements (e.g. on the order of seconds or less)

   - You are willing to have resources continually allocated

   `AsyncEndpoints` are good if:

   - You want to save on compute costs

   - Your inference code takes a long time to run

   - Your latency requirements are on the order of minutes.

   `BatchJobs` are good if:

   - You know there is a large batch of inputs ahead of time

   - You want to process data in an offline fashion

   Steps to deploy your model via Scale Launch:

   1. First, you create and upload a `ModelBundle`. Pass your trained model as well as pre-/post-processing code to
   the Scale Launch Python SDK, and we'll create a model bundle based on the code and store it in our Bundle Store.

   2. Then, you create a `ModelEndpoint`. Pass a `ModelBundle` as well as infrastructure settings such as #GPUs to our SDK.
   This provisions resources on Scale's cluster dedicated to your `ModelEndpoint`.

   3. Lastly, you make requests to the `ModelEndpoint`. You can make requests through the Python SDK, or make HTTP requests directly
   to Scale.

   .. image:: /../docs/images/request_lifecycle.png
       :width: 400px

   TODO: link some example colab notebook



Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   request_validation/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   launch.AsyncEndpoint
   launch.AsyncEndpointBatchResponse
   launch.Connection
   launch.LaunchClient
   launch.ModelBundle




.. py:class:: AsyncEndpoint(model_endpoint, client)



   A higher level abstraction for a Model Endpoint.

   :param model_endpoint: ModelEndpoint object.
   :param client: A LaunchClient object

   .. py:method:: async_request(self, url)
      :abstractmethod:
      :async:

      Makes an async request to the endpoint. Polls the endpoint under the hood, but provides async/await semantics
      on top.

      :param url: A url that points to a file containing model input.
                  Must be accessible by Scale Launch, hence it needs to either be public or a signedURL.

      :returns: A signedUrl that contains a cloudpickled Python object, the result of running inference on the model input
                Example output:
                    `https://foo.s3.us-west-2.amazonaws.com/bar/baz/qux?xyzzy`


   .. py:method:: predict_batch(self, requests)

      Runs inference on the data items specified by urls. Returns a AsyncEndpointResponse.

      :param requests: List of EndpointRequests. Request_ids must all be distinct.

      :returns: an AsyncEndpointResponse keeping track of the inference requests made


   .. py:method:: status(self)
      :abstractmethod:

      Gets the status of the Endpoint.
      TODO this functionality currently does not exist on the server.



.. py:class:: AsyncEndpointBatchResponse(client, request_ids)

   Currently represents a list of async inference requests to a specific endpoint. Keeps track of the requests made,
   and gives a way to poll for their status.

   Invariant: set keys for self.request_ids and self.responses are equal

   idk about this abstraction tbh, could use a redesign maybe?

   Also batch inference sort of removes the need for much of the complication in here


   .. py:method:: get_responses(self)

      Returns a dictionary, where each key is the request_id for an EndpointRequest passed in, and the corresponding
      object at that key is the corresponding EndpointResponse.


   .. py:method:: is_done(self, poll=True)

      Checks if all the tasks from this round of requests are done, according to
      the internal state of this object.
      Optionally polls the endpoints to pick up new tasks that may have finished.


   .. py:method:: poll_endpoints(self)

      Runs one round of polling the endpoint for async task results


   .. py:method:: wait(self)
      :abstractmethod:
      :async:

      Waits for inference results to complete. Provides async/await semantics, but under the hood does polling.
      TODO: we'd need to implement some lower level asyncio request code



.. py:class:: Connection(api_key, endpoint = None)

   Wrapper of HTTP requests to the Launch endpoint.

   .. py:method:: make_request(self, payload, route, requests_command=requests.post)

      Makes a request to Launch endpoint and logs a warning if not
      successful.

      :param payload: given payload
      :param route: route for the request
      :param requests_command: requests.post, requests.get, requests.delete
      :return: response JSON



.. py:class:: LaunchClient(api_key, endpoint = None, self_hosted = False)

   Scale Launch Python Client extension.

   Initializes a Scale Launch Client.

   :param api_key: Your Scale API key
   :param endpoint: The Scale Launch Endpoint (this should not need to be changed)
   :param self_hosted: True iff you are connecting to a self-hosted Scale Launch

   .. py:method:: async_request(self, endpoint_id, url = None, args = None, return_pickled = True)

      Not recommended to use this, instead we recommend to use functions provided by AsyncEndpoint.
      Makes a request to the Async Model Endpoint at endpoint_id, and immediately returns a key that can be used to retrieve
      the result of inference at a later time.
      Endpoint

      :param endpoint_id: The id of the endpoint to make the request to
      :param url: A url that points to a file containing model input.
                  Must be accessible by Scale Launch, hence it needs to either be public or a signedURL.
      :param args: A dictionary of arguments to the ModelBundle's predict function.
                   Must be json-serializable, i.e. composed of str, int, float, etc.
                   If your `predict` function has signature `predict(foo, bar)`, then args should be a dictionary with
                   keys `foo` and `bar`. Exactly one of url and args must be specified.
      :param return_pickled: Whether the python object returned is pickled, or directly written to the file returned.

      :returns: An id/key that can be used to fetch inference results at a later time.
                Example output:
                    `abcabcab-cabc-abca-0123456789ab`


   .. py:method:: batch_async_request(self, endpoint_id, urls)
      :abstractmethod:

      Sends a batch inference request to the Model Endpoint at endpoint_id, returns a key that can be used to retrieve
      the results of inference at a later time.

      :param endpoint_id: The id of the endpoint to make the request to
      :param urls: A list of urls, each pointing to a file containing model input.
                   Must be accessible by Scale Launch, hence urls need to either be public or signedURLs.

      :returns: An id/key that can be used to fetch inference results at a later time


   .. py:method:: create_model_bundle(self, model_bundle_name, env_params, *, load_predict_fn = None, predict_fn_or_cls = None, requirements = None, model = None, load_model_fn = None, bundle_url = None, app_config = None, globals_copy = None)

      Grabs a s3 signed url and uploads a model bundle to Scale Launch.

      A model bundle consists of exactly {predict_fn_or_cls}, {load_predict_fn + model}, or {load_predict_fn + load_model_fn}.
      Pre/post-processing code can be included inside load_predict_fn/model or in predict_fn_or_cls call.

      :param model_bundle_name: Name of model bundle you want to create. This acts as a unique identifier.
      :param predict_fn_or_cls: Function or a Callable class that runs end-to-end (pre/post processing and model inference) on the call.
                                I.e. `predict_fn_or_cls(REQUEST) -> RESPONSE`.
      :param model: Typically a trained Neural Network, e.g. a Pytorch module
      :param load_predict_fn: Function that when called with model, returns a function that carries out inference
                              I.e. `load_predict_fn(model) -> func; func(REQUEST) -> RESPONSE`
      :param load_model_fn: Function that when run, loads a model, e.g. a Pytorch module
                            I.e. `load_predict_fn(load_model_fn()) -> func; func(REQUEST) -> RESPONSE`
      :param bundle_url: Only for self-hosted mode. Desired location of bundle.
      :param Overrides any value given by self.bundle_location_fn:
      :param requirements: A list of python package requirements, e.g.
                           ["tensorflow==2.3.0", "tensorflow-hub==0.11.0"]. If no list has been passed, will default to the currently
                           imported list of packages.
      :param app_config: Either a Dictionary that represents a YAML file contents or a local path to a YAML file.
      :param env_params: A dictionary that dictates environment information e.g.
                         the use of pytorch or tensorflow, which cuda/cudnn versions to use.
                         Specifically, the dictionary should contain the following keys:
                         "framework_type": either "tensorflow" or "pytorch".
                         "pytorch_version": Version of pytorch, e.g. "1.5.1", "1.7.0", etc. Only applicable if framework_type is pytorch
                         "cuda_version": Version of cuda used, e.g. "11.0".
                         "cudnn_version" Version of cudnn used, e.g. "cudnn8-devel".
                         "tensorflow_version": Version of tensorflow, e.g. "2.3.0". Only applicable if framework_type is tensorflow
      :param globals_copy: Dictionary of the global symbol table. Normally provided by `globals()` built-in function.


   .. py:method:: create_model_bundle_from_dir(self, model_bundle_name, base_path, requirements_path, env_params, load_predict_fn_module_path, load_model_fn_module_path, app_config = None)

      Packages up code from a local filesystem folder and uploads that as a bundle to Scale Launch.
      In this mode, a bundle is just local code instead of a serialized object.

      :param model_bundle_name: Name of model bundle you want to create. This acts as a unique identifier.
      :param base_path: The path on the local filesystem where the bundle code lives.
      :param requirements_path: A path on the local filesystem where a requirements.txt file lives.
      :param env_params: A dictionary that dictates environment information e.g.
                         the use of pytorch or tensorflow, which cuda/cudnn versions to use.
                         Specifically, the dictionary should contain the following keys:
                         "framework_type": either "tensorflow" or "pytorch".
                         "pytorch_version": Version of pytorch, e.g. "1.5.1", "1.7.0", etc. Only applicable if framework_type is pytorch
                         "cuda_version": Version of cuda used, e.g. "11.0".
                         "cudnn_version" Version of cudnn used, e.g. "cudnn8-devel".
                         "tensorflow_version": Version of tensorflow, e.g. "2.3.0". Only applicable if framework_type is tensorflow
      :param load_predict_fn_module_path: A python module path within base_path for a function that, when called with the output of
                                          load_model_fn_module_path, returns a function that carries out inference.
      :param load_model_fn_module_path: A python module path within base_path for a function that returns a model. The output feeds into
                                        the function located at load_predict_fn_module_path.
      :param app_config: Either a Dictionary that represents a YAML file contents or a local path to a YAML file.


   .. py:method:: create_model_endpoint(self, endpoint_name, model_bundle, cpus = 3, memory = '8Gi', gpus = 0, min_workers = 1, max_workers = 1, per_worker = 1, gpu_type = None, endpoint_type = 'sync', update_if_exists = False)

      Creates a Model Endpoint that is able to serve requests.
      Corresponds to POST/PUT endpoints

      :param endpoint_name: Name of model endpoint. Must be unique.
      :param model_bundle: The ModelBundle that you want your Model Endpoint to serve
      :param cpus: Number of cpus each worker should get, e.g. 1, 2, etc.
      :param memory: Amount of memory each worker should get, e.g. "4Gi", "512Mi", etc.
      :param gpus: Number of gpus each worker should get, e.g. 0, 1, etc.
      :param min_workers: Minimum number of workers for model endpoint
      :param max_workers: Maximum number of workers for model endpoint
      :param per_worker: An autoscaling parameter. Use this to make a tradeoff between latency and costs,
                         a lower per_worker will mean more workers are created for a given workload
      :param gpu_type: If specifying a non-zero number of gpus, this controls the type of gpu requested. Current options are
                       "nvidia-tesla-t4" for NVIDIA T4s, or "nvidia-tesla-v100" for NVIDIA V100s.
      :param endpoint_type: Either "sync" or "async". Type of endpoint we want to instantiate.

      :returns: A Endpoint object that can be used to make requests to the endpoint.


   .. py:method:: delete_model_bundle(self, model_bundle)

      Deletes the model bundle on the server.


   .. py:method:: delete_model_endpoint(self, model_endpoint)

      Deletes a model endpoint.


   .. py:method:: edit_model_endpoint(self, endpoint_name, model_bundle = None, cpus = None, memory = None, gpus = None, min_workers = None, max_workers = None, per_worker = None, gpu_type = None)

      Edit an existing model endpoint


   .. py:method:: get_async_response(self, async_task_id)

      Not recommended to use this, instead we recommend to use functions provided by AsyncEndpoint.
      Gets inference results from a previously created task.

      :param async_task_id: The id/key returned from a previous invocation of async_request.

      :returns: A dictionary that contains task status and optionally a result url or result if the task has completed.
                Result url or result will be returned if the task has succeeded. Will return a result url iff `return_pickled`
                was set to True on task creation.
                Dictionary's keys are as follows:
                state: 'PENDING' or 'SUCCESS' or 'FAILURE'
                result_url: a url pointing to inference results. This url is accessible for 12 hours after the request has been made.
                result: the value returned by the endpoint's `predict` function, serialized as json
                Example output:
                    `{'state': 'SUCCESS', 'result_url': 'https://foo.s3.us-west-2.amazonaws.com/bar/baz/qux?xyzzy'}`

      TODO: do we want to read the results from here as well? i.e. translate result_url into a python object


   .. py:method:: get_batch_async_response(self, batch_async_task_id)
      :abstractmethod:

      TODO not sure about how the batch task returns an identifier for the batch.
      Gets inference results from a previously created batch task.

      :param batch_async_task_id: An id representing the batch task job

      :returns: TODO Something similar to a list of signed s3URLs


   .. py:method:: get_model_bundle(self, bundle_name)

      Returns a Model Bundle object specified by `bundle_name`.
      :returns: A ModelBundle object


   .. py:method:: list_model_bundles(self)

      Returns a list of model bundles that the user owns.

      :returns: A list of ModelBundle objects


   .. py:method:: list_model_endpoints(self)

      Lists all model endpoints that the user owns.
      TODO: single get_model_endpoint(self)? route doesn't exist serverside I think

      :returns: A list of ModelEndpoint objects


   .. py:method:: read_endpoint_creation_logs(self, endpoint_name)

      Get builder logs as text.


   .. py:method:: register_bundle_location_fn(self, bundle_location_fn)

      For self-hosted mode only. Registers a function that gives a location for a model bundle. Should give different
      locations each time. This function is called as bundle_location_fn(), and should return a bundle_url that
      register_upload_bundle_fn can take.

      Strictly, bundle_location_fn() does not need to return a str. The only requirement is that if bundle_location_fn
      returns a value of type T, then upload_bundle_fn() takes in an object of type T as its second argument
      (i.e. bundle_url).

      :param bundle_location_fn: Function that generates bundle_urls for upload_bundle_fn.


   .. py:method:: register_endpoint_auth_decorator(self, endpoint_auth_decorator_fn)

      For self-hosted mode only. Registers a function that modifies the endpoint creation payload to include
      required fields for self-hosting.


   .. py:method:: register_upload_bundle_fn(self, upload_bundle_fn)

      For self-hosted mode only. Registers a function that handles model bundle upload. This function is called as

      upload_bundle_fn(serialized_bundle, bundle_url)

      This function should directly write the contents of serialized_bundle as a binary string into bundle_url.

      See register_bundle_location_fn for more notes on the signature of upload_bundle_fn

      :param upload_bundle_fn: Function that takes in a serialized bundle, and uploads that bundle to an appropriate
                               location. Only needed for self-hosted mode.


   .. py:method:: sync_request(self, endpoint_id, url = None, args = None, return_pickled = True)

      Not recommended for use, instead use functions provided by SyncEndpoint
      Makes a request to the Sync Model Endpoint at endpoint_id, and blocks until request completion or timeout.
      Endpoint at endpoint_id must be a SyncEndpoint, otherwise this request will fail.

      :param endpoint_id: The id of the endpoint to make the request to
      :param url: A url that points to a file containing model input.
                  Must be accessible by Scale Launch, hence it needs to either be public or a signedURL.
      :param args: A dictionary of arguments to the `predict` function defined in your model bundle.
                   Must be json-serializable, i.e. composed of str, int, float, etc.
                   If your `predict` function has signature `predict(foo, bar)`, then args should be a dictionary with
                   keys `foo` and `bar`. Exactly one of url and args must be specified.
      :param return_pickled: Whether the python object returned is pickled, or directly written to the file returned.

      :returns: A dictionary with key either "result_url" or "result", depending on the value of `return_pickled`.
                If `return_pickled` is true, the key will be "result_url",
                and the value is a signedUrl that contains a cloudpickled Python object,
                the result of running inference on the model input.
                Example output:
                    `https://foo.s3.us-west-2.amazonaws.com/bar/baz/qux?xyzzy`

                Otherwise, if `return_pickled` is false, the key will be "result",
                and the value is the output of the endpoint's `predict` function, serialized as json.



.. py:class:: ModelBundle

   Represents a ModelBundle.


