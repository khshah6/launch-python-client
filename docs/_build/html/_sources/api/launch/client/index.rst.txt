:orphan:

:py:mod:`launch.client`
=======================

.. py:module:: launch.client


Module Contents
---------------

Classes
~~~~~~~

.. autoapisummary::

   launch.client.LaunchClient




.. py:class:: LaunchClient(api_key, endpoint = None, self_hosted = False)

   Scale Launch Python Client extension.

   Initializes a Scale Launch Client.

   :param api_key: Your Scale API key
   :param endpoint: The Scale Launch Endpoint (this should not need to be changed)
   :param self_hosted: True iff you are connecting to a self-hosted Scale Launch

   .. py:method:: async_request(self, endpoint_id, url = None, args = None, return_pickled = True)

      Not recommended to use this, instead we recommend to use functions provided by AsyncEndpoint.
      Makes a request to the Async Model Endpoint at endpoint_id, and immediately returns a key that can be used to retrieve
      the result of inference at a later time.
      Endpoint

      :param endpoint_id: The id of the endpoint to make the request to
      :param url: A url that points to a file containing model input.
                  Must be accessible by Scale Launch, hence it needs to either be public or a signedURL.
      :param args: A dictionary of arguments to the ModelBundle's predict function.
                   Must be json-serializable, i.e. composed of str, int, float, etc.
                   If your `predict` function has signature `predict(foo, bar)`, then args should be a dictionary with
                   keys `foo` and `bar`. Exactly one of url and args must be specified.
      :param return_pickled: Whether the python object returned is pickled, or directly written to the file returned.

      :returns: An id/key that can be used to fetch inference results at a later time.
                Example output:
                    `abcabcab-cabc-abca-0123456789ab`


   .. py:method:: batch_async_request(self, endpoint_id, urls)
      :abstractmethod:

      Sends a batch inference request to the Model Endpoint at endpoint_id, returns a key that can be used to retrieve
      the results of inference at a later time.

      :param endpoint_id: The id of the endpoint to make the request to
      :param urls: A list of urls, each pointing to a file containing model input.
                   Must be accessible by Scale Launch, hence urls need to either be public or signedURLs.

      :returns: An id/key that can be used to fetch inference results at a later time


   .. py:method:: create_model_bundle(self, model_bundle_name, env_params, *, load_predict_fn = None, predict_fn_or_cls = None, requirements = None, model = None, load_model_fn = None, bundle_url = None, app_config = None, globals_copy = None)

      Grabs a s3 signed url and uploads a model bundle to Scale Launch.

      A model bundle consists of exactly {predict_fn_or_cls}, {load_predict_fn + model}, or {load_predict_fn + load_model_fn}.
      Pre/post-processing code can be included inside load_predict_fn/model or in predict_fn_or_cls call.

      :param model_bundle_name: Name of model bundle you want to create. This acts as a unique identifier.
      :param predict_fn_or_cls: Function or a Callable class that runs end-to-end (pre/post processing and model inference) on the call.
                                I.e. `predict_fn_or_cls(REQUEST) -> RESPONSE`.
      :param model: Typically a trained Neural Network, e.g. a Pytorch module
      :param load_predict_fn: Function that when called with model, returns a function that carries out inference
                              I.e. `load_predict_fn(model) -> func; func(REQUEST) -> RESPONSE`
      :param load_model_fn: Function that when run, loads a model, e.g. a Pytorch module
                            I.e. `load_predict_fn(load_model_fn()) -> func; func(REQUEST) -> RESPONSE`
      :param bundle_url: Only for self-hosted mode. Desired location of bundle.
      :param Overrides any value given by self.bundle_location_fn:
      :param requirements: A list of python package requirements, e.g.
                           ["tensorflow==2.3.0", "tensorflow-hub==0.11.0"]. If no list has been passed, will default to the currently
                           imported list of packages.
      :param app_config: Either a Dictionary that represents a YAML file contents or a local path to a YAML file.
      :param env_params: A dictionary that dictates environment information e.g.
                         the use of pytorch or tensorflow, which cuda/cudnn versions to use.
                         Specifically, the dictionary should contain the following keys:
                         "framework_type": either "tensorflow" or "pytorch".
                         "pytorch_version": Version of pytorch, e.g. "1.5.1", "1.7.0", etc. Only applicable if framework_type is pytorch
                         "cuda_version": Version of cuda used, e.g. "11.0".
                         "cudnn_version" Version of cudnn used, e.g. "cudnn8-devel".
                         "tensorflow_version": Version of tensorflow, e.g. "2.3.0". Only applicable if framework_type is tensorflow
      :param globals_copy: Dictionary of the global symbol table. Normally provided by `globals()` built-in function.


   .. py:method:: create_model_bundle_from_dir(self, model_bundle_name, base_path, requirements_path, env_params, load_predict_fn_module_path, load_model_fn_module_path, app_config = None)

      Packages up code from a local filesystem folder and uploads that as a bundle to Scale Launch.
      In this mode, a bundle is just local code instead of a serialized object.

      :param model_bundle_name: Name of model bundle you want to create. This acts as a unique identifier.
      :param base_path: The path on the local filesystem where the bundle code lives.
      :param requirements_path: A path on the local filesystem where a requirements.txt file lives.
      :param env_params: A dictionary that dictates environment information e.g.
                         the use of pytorch or tensorflow, which cuda/cudnn versions to use.
                         Specifically, the dictionary should contain the following keys:
                         "framework_type": either "tensorflow" or "pytorch".
                         "pytorch_version": Version of pytorch, e.g. "1.5.1", "1.7.0", etc. Only applicable if framework_type is pytorch
                         "cuda_version": Version of cuda used, e.g. "11.0".
                         "cudnn_version" Version of cudnn used, e.g. "cudnn8-devel".
                         "tensorflow_version": Version of tensorflow, e.g. "2.3.0". Only applicable if framework_type is tensorflow
      :param load_predict_fn_module_path: A python module path within base_path for a function that, when called with the output of
                                          load_model_fn_module_path, returns a function that carries out inference.
      :param load_model_fn_module_path: A python module path within base_path for a function that returns a model. The output feeds into
                                        the function located at load_predict_fn_module_path.
      :param app_config: Either a Dictionary that represents a YAML file contents or a local path to a YAML file.


   .. py:method:: create_model_endpoint(self, endpoint_name, model_bundle, cpus = 3, memory = '8Gi', gpus = 0, min_workers = 1, max_workers = 1, per_worker = 1, gpu_type = None, endpoint_type = 'sync', update_if_exists = False)

      Creates a Model Endpoint that is able to serve requests.
      Corresponds to POST/PUT endpoints

      :param endpoint_name: Name of model endpoint. Must be unique.
      :param model_bundle: The ModelBundle that you want your Model Endpoint to serve
      :param cpus: Number of cpus each worker should get, e.g. 1, 2, etc.
      :param memory: Amount of memory each worker should get, e.g. "4Gi", "512Mi", etc.
      :param gpus: Number of gpus each worker should get, e.g. 0, 1, etc.
      :param min_workers: Minimum number of workers for model endpoint
      :param max_workers: Maximum number of workers for model endpoint
      :param per_worker: An autoscaling parameter. Use this to make a tradeoff between latency and costs,
                         a lower per_worker will mean more workers are created for a given workload
      :param gpu_type: If specifying a non-zero number of gpus, this controls the type of gpu requested. Current options are
                       "nvidia-tesla-t4" for NVIDIA T4s, or "nvidia-tesla-v100" for NVIDIA V100s.
      :param endpoint_type: Either "sync" or "async". Type of endpoint we want to instantiate.

      :returns: A Endpoint object that can be used to make requests to the endpoint.


   .. py:method:: delete_model_bundle(self, model_bundle)

      Deletes the model bundle on the server.


   .. py:method:: delete_model_endpoint(self, model_endpoint)

      Deletes a model endpoint.


   .. py:method:: edit_model_endpoint(self, endpoint_name, model_bundle = None, cpus = None, memory = None, gpus = None, min_workers = None, max_workers = None, per_worker = None, gpu_type = None)

      Edit an existing model endpoint


   .. py:method:: get_async_response(self, async_task_id)

      Not recommended to use this, instead we recommend to use functions provided by AsyncEndpoint.
      Gets inference results from a previously created task.

      :param async_task_id: The id/key returned from a previous invocation of async_request.

      :returns: A dictionary that contains task status and optionally a result url or result if the task has completed.
                Result url or result will be returned if the task has succeeded. Will return a result url iff `return_pickled`
                was set to True on task creation.
                Dictionary's keys are as follows:
                state: 'PENDING' or 'SUCCESS' or 'FAILURE'
                result_url: a url pointing to inference results. This url is accessible for 12 hours after the request has been made.
                result: the value returned by the endpoint's `predict` function, serialized as json
                Example output:
                    `{'state': 'SUCCESS', 'result_url': 'https://foo.s3.us-west-2.amazonaws.com/bar/baz/qux?xyzzy'}`

      TODO: do we want to read the results from here as well? i.e. translate result_url into a python object


   .. py:method:: get_batch_async_response(self, batch_async_task_id)
      :abstractmethod:

      TODO not sure about how the batch task returns an identifier for the batch.
      Gets inference results from a previously created batch task.

      :param batch_async_task_id: An id representing the batch task job

      :returns: TODO Something similar to a list of signed s3URLs


   .. py:method:: get_model_bundle(self, bundle_name)

      Returns a Model Bundle object specified by `bundle_name`.
      :returns: A ModelBundle object


   .. py:method:: list_model_bundles(self)

      Returns a list of model bundles that the user owns.

      :returns: A list of ModelBundle objects


   .. py:method:: list_model_endpoints(self)

      Lists all model endpoints that the user owns.
      TODO: single get_model_endpoint(self)? route doesn't exist serverside I think

      :returns: A list of ModelEndpoint objects


   .. py:method:: read_endpoint_creation_logs(self, endpoint_name)

      Get builder logs as text.


   .. py:method:: register_bundle_location_fn(self, bundle_location_fn)

      For self-hosted mode only. Registers a function that gives a location for a model bundle. Should give different
      locations each time. This function is called as bundle_location_fn(), and should return a bundle_url that
      register_upload_bundle_fn can take.

      Strictly, bundle_location_fn() does not need to return a str. The only requirement is that if bundle_location_fn
      returns a value of type T, then upload_bundle_fn() takes in an object of type T as its second argument
      (i.e. bundle_url).

      :param bundle_location_fn: Function that generates bundle_urls for upload_bundle_fn.


   .. py:method:: register_endpoint_auth_decorator(self, endpoint_auth_decorator_fn)

      For self-hosted mode only. Registers a function that modifies the endpoint creation payload to include
      required fields for self-hosting.


   .. py:method:: register_upload_bundle_fn(self, upload_bundle_fn)

      For self-hosted mode only. Registers a function that handles model bundle upload. This function is called as

      upload_bundle_fn(serialized_bundle, bundle_url)

      This function should directly write the contents of serialized_bundle as a binary string into bundle_url.

      See register_bundle_location_fn for more notes on the signature of upload_bundle_fn

      :param upload_bundle_fn: Function that takes in a serialized bundle, and uploads that bundle to an appropriate
                               location. Only needed for self-hosted mode.


   .. py:method:: sync_request(self, endpoint_id, url = None, args = None, return_pickled = True)

      Not recommended for use, instead use functions provided by SyncEndpoint
      Makes a request to the Sync Model Endpoint at endpoint_id, and blocks until request completion or timeout.
      Endpoint at endpoint_id must be a SyncEndpoint, otherwise this request will fail.

      :param endpoint_id: The id of the endpoint to make the request to
      :param url: A url that points to a file containing model input.
                  Must be accessible by Scale Launch, hence it needs to either be public or a signedURL.
      :param args: A dictionary of arguments to the `predict` function defined in your model bundle.
                   Must be json-serializable, i.e. composed of str, int, float, etc.
                   If your `predict` function has signature `predict(foo, bar)`, then args should be a dictionary with
                   keys `foo` and `bar`. Exactly one of url and args must be specified.
      :param return_pickled: Whether the python object returned is pickled, or directly written to the file returned.

      :returns: A dictionary with key either "result_url" or "result", depending on the value of `return_pickled`.
                If `return_pickled` is true, the key will be "result_url",
                and the value is a signedUrl that contains a cloudpickled Python object,
                the result of running inference on the model input.
                Example output:
                    `https://foo.s3.us-west-2.amazonaws.com/bar/baz/qux?xyzzy`

                Otherwise, if `return_pickled` is false, the key will be "result",
                and the value is the output of the endpoint's `predict` function, serialized as json.



