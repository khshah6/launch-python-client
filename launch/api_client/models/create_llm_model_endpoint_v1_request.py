# coding: utf-8

"""
    launch

    No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)

    The version of the OpenAPI document: 1.0.0
    Generated by OpenAPI Generator (https://openapi-generator.tech)

    Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations

import json
import pprint
import re  # noqa: F401
from typing import Any, ClassVar, Dict, List, Optional, Set

from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    StrictBool,
    StrictInt,
    StrictStr,
)
from typing_extensions import Annotated, Self

from launch.api_client.models.callback_auth import CallbackAuth
from launch.api_client.models.cpus import Cpus
from launch.api_client.models.gpu_type import GpuType
from launch.api_client.models.llm_inference_framework import (
    LLMInferenceFramework,
)
from launch.api_client.models.llm_source import LLMSource
from launch.api_client.models.memory import Memory
from launch.api_client.models.model_endpoint_type import ModelEndpointType
from launch.api_client.models.quantization import Quantization
from launch.api_client.models.storage import Storage


class CreateLLMModelEndpointV1Request(BaseModel):
    """
    CreateLLMModelEndpointV1Request
    """  # noqa: E501

    billing_tags: Optional[Dict[str, Any]] = None
    checkpoint_path: Optional[StrictStr] = None
    cpus: Optional[Cpus] = None
    default_callback_auth: Optional[CallbackAuth] = None
    default_callback_url: Optional[Annotated[str, Field(min_length=1, strict=True, max_length=2083)]] = None
    endpoint_type: Optional[ModelEndpointType] = None
    gpu_type: Optional[GpuType] = None
    gpus: Optional[StrictInt] = None
    high_priority: Optional[StrictBool] = None
    inference_framework: Optional[LLMInferenceFramework] = None
    inference_framework_image_tag: Optional[StrictStr] = "latest"
    labels: Dict[str, StrictStr]
    max_workers: StrictInt
    memory: Optional[Memory] = None
    metadata: Dict[str, Any]
    min_workers: StrictInt
    model_name: StrictStr
    name: StrictStr
    num_shards: Optional[StrictInt] = 1
    optimize_costs: Optional[StrictBool] = None
    per_worker: StrictInt
    post_inference_hooks: Optional[List[StrictStr]] = None
    prewarm: Optional[StrictBool] = None
    public_inference: Optional[StrictBool] = True
    quantize: Optional[Quantization] = None
    source: Optional[LLMSource] = None
    storage: Optional[Storage] = None
    __properties: ClassVar[List[str]] = [
        "billing_tags",
        "checkpoint_path",
        "cpus",
        "default_callback_auth",
        "default_callback_url",
        "endpoint_type",
        "gpu_type",
        "gpus",
        "high_priority",
        "inference_framework",
        "inference_framework_image_tag",
        "labels",
        "max_workers",
        "memory",
        "metadata",
        "min_workers",
        "model_name",
        "name",
        "num_shards",
        "optimize_costs",
        "per_worker",
        "post_inference_hooks",
        "prewarm",
        "public_inference",
        "quantize",
        "source",
        "storage",
    ]

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of CreateLLMModelEndpointV1Request from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of cpus
        if self.cpus:
            _dict["cpus"] = self.cpus.to_dict()
        # override the default output from pydantic by calling `to_dict()` of default_callback_auth
        if self.default_callback_auth:
            _dict["default_callback_auth"] = self.default_callback_auth.to_dict()
        # override the default output from pydantic by calling `to_dict()` of memory
        if self.memory:
            _dict["memory"] = self.memory.to_dict()
        # override the default output from pydantic by calling `to_dict()` of storage
        if self.storage:
            _dict["storage"] = self.storage.to_dict()
        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of CreateLLMModelEndpointV1Request from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate(
            {
                "billing_tags": obj.get("billing_tags"),
                "checkpoint_path": obj.get("checkpoint_path"),
                "cpus": Cpus.from_dict(obj["cpus"]) if obj.get("cpus") is not None else None,
                "default_callback_auth": CallbackAuth.from_dict(obj["default_callback_auth"])
                if obj.get("default_callback_auth") is not None
                else None,
                "default_callback_url": obj.get("default_callback_url"),
                "endpoint_type": obj.get("endpoint_type"),
                "gpu_type": obj.get("gpu_type"),
                "gpus": obj.get("gpus"),
                "high_priority": obj.get("high_priority"),
                "inference_framework": obj.get("inference_framework"),
                "inference_framework_image_tag": obj.get("inference_framework_image_tag")
                if obj.get("inference_framework_image_tag") is not None
                else "latest",
                "labels": obj.get("labels"),
                "max_workers": obj.get("max_workers"),
                "memory": Memory.from_dict(obj["memory"]) if obj.get("memory") is not None else None,
                "metadata": obj.get("metadata"),
                "min_workers": obj.get("min_workers"),
                "model_name": obj.get("model_name"),
                "name": obj.get("name"),
                "num_shards": obj.get("num_shards") if obj.get("num_shards") is not None else 1,
                "optimize_costs": obj.get("optimize_costs"),
                "per_worker": obj.get("per_worker"),
                "post_inference_hooks": obj.get("post_inference_hooks"),
                "prewarm": obj.get("prewarm"),
                "public_inference": obj.get("public_inference") if obj.get("public_inference") is not None else True,
                "quantize": obj.get("quantize"),
                "source": obj.get("source"),
                "storage": Storage.from_dict(obj["storage"]) if obj.get("storage") is not None else None,
            }
        )
        return _obj
