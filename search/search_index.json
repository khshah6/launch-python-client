{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Scale Launch","text":"<p>Simple, scalable, and high performance ML service deployment in python.</p>"},{"location":"#example","title":"Example","text":"Launch Usage<pre><code>import os\nimport time\nfrom launch import LaunchClient\nfrom launch import EndpointRequest\nfrom pydantic import BaseModel\nfrom rich import print\nclass MyRequestSchema(BaseModel):\nx: int\ny: str\nclass MyResponseSchema(BaseModel):\n__root__: int\ndef my_load_predict_fn(model):\ndef returns_model_of_x_plus_len_of_y(x: int, y: str) -&gt; int:\n\"\"\"MyRequestSchema -&gt; MyResponseSchema\"\"\"\nassert isinstance(x, int) and isinstance(y, str)\nreturn model(x) + len(y)\nreturn returns_model_of_x_plus_len_of_y\ndef my_model(x):\nreturn x * 2\nENV_PARAMS = {\n\"framework_type\": \"pytorch\",\n\"pytorch_image_tag\": \"1.7.1-cuda11.0-cudnn8-runtime\",\n}\nBUNDLE_PARAMS = {\n\"model_bundle_name\": \"test-bundle\",\n\"model\": my_model,\n\"load_predict_fn\": my_load_predict_fn,\n\"env_params\": ENV_PARAMS,\n\"requirements\": [\"pytest==7.2.1\", \"numpy\"],  # list your requirements here\n\"request_schema\": MyRequestSchema,\n\"response_schema\": MyResponseSchema,\n}\nENDPOINT_PARAMS = {\n\"endpoint_name\": \"demo-endpoint\",\n\"model_bundle\": \"test-bundle\",\n\"cpus\": 1,\n\"min_workers\": 0,\n\"endpoint_type\": \"async\",\n\"update_if_exists\": True,\n\"labels\": {\n\"team\": \"MY_TEAM\",\n\"product\": \"launch\",\n}\n}\ndef predict_on_endpoint(request: MyRequestSchema) -&gt; MyResponseSchema:\n# Wait for the endpoint to be ready first before submitting a task\nendpoint = client.get_model_endpoint(endpoint_name=\"demo-endpoint\")\nwhile endpoint.status() != \"READY\":\ntime.sleep(10)\nendpoint_request = EndpointRequest(args=request.dict(), return_pickled=False)\nfuture = endpoint.predict(request=endpoint_request)\nraw_response = future.get()\nresponse = MyResponseSchema.parse_raw(raw_response.result)\nreturn response\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nclient.create_model_bundle(**BUNDLE_PARAMS)\nendpoint = client.create_model_endpoint(**ENDPOINT_PARAMS)\nrequest = MyRequestSchema(x=5, y=\"hello\")\nresponse = predict_on_endpoint(request)\nprint(response)\n\"\"\"\nMyResponseSchema(__root__=10)\n\"\"\"\n</code></pre> <p>What's going on here:</p> <ul> <li>First we use <code>pydantic</code> to define our request and response   schemas, <code>MyRequestSchema</code> and <code>MyResponseSchema</code>. These schemas are used to generate the API   documentation for our models.</li> <li>Next we define the the <code>model</code> and the <code>load_predict_fn</code>, which tells Launch   how to load our model and how to make predictions with it. In this case,   we're just returning a function that adds the length of the string <code>y</code> to    <code>model(x)</code>, where <code>model</code> doubles the integer <code>x</code>.</li> <li>We then define the model bundle by specifying the <code>load_predict_fn</code>, the <code>request_schema</code>, and the   <code>response_schema</code>. We also specify the <code>env_params</code>, which tell Launch environment settings like    the base image to use. In this case, we're using a PyTorch image.</li> <li>Next, we create the model endpoint, which is the API that we'll use to make predictions. We   specify the <code>model_bundle</code> that we created above, and we specify the <code>endpoint_type</code>, which tells   Launch whether to use a synchronous or asynchronous endpoint. In this case, we're using an   asynchronous endpoint, which means that we can make predictions and return immediately with a   <code>future</code> object. We can then use the <code>future</code> object to get the prediction result later.</li> <li>Finally, we make a prediction by calling <code>predict_on_endpoint</code> with a <code>MyRequestSchema</code> object.   This function first waits for the endpoint to be ready, then it submits a prediction request to   the endpoint. It then waits for the prediction result and returns it.</li> </ul> <p>Notice that we specified <code>min_workers=0</code>, meaning that the endpoint will scale down to 0 workers when it's not being used.</p>"},{"location":"#installation","title":"Installation","text":"<p>To use Scale Launch, first install it using <code>pip</code>:</p> Installation<pre><code>pip install -U scale-launch\n</code></pre>"},{"location":"cli/","title":"CLI","text":"<p>Launch comes with a CLI for listing bundles / endpoints, editing endpoints, and sending tasks to endpoints.</p> <p>The CLI can be used as <code>scale-launch ...</code>.</p>"},{"location":"cli/#help","title":"Help","text":"<p>Run <code>scale-launch --help</code> for more options.</p> scale-launch --help<pre><code>    This is the command line interface (CLI) package for Scale Launch.\n\n       \u2588\u2588\u2557      \u2588\u2588\u2588\u2588\u2588\u2557 \u2588\u2588\u2557   \u2588\u2588\u2557\u2588\u2588\u2588\u2557   \u2588\u2588\u2557 \u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2557  \u2588\u2588\u2557\n       \u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2557\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2588\u2588\u2557  \u2588\u2588\u2551\u2588\u2588\u2554\u2550\u2550\u2550\u2550\u255d\u2588\u2588\u2551  \u2588\u2588\u2551\n       \u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2554\u2588\u2588\u2557 \u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2551\n       \u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\u2588\u2588\u2551   \u2588\u2588\u2551\u2588\u2588\u2551\u255a\u2588\u2588\u2557\u2588\u2588\u2551\u2588\u2588\u2551     \u2588\u2588\u2554\u2550\u2550\u2588\u2588\u2551\n       \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2554\u255d\u2588\u2588\u2551 \u255a\u2588\u2588\u2588\u2588\u2551\u255a\u2588\u2588\u2588\u2588\u2588\u2588\u2557\u2588\u2588\u2551  \u2588\u2588\u2551\n       \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d \u255a\u2550\u255d  \u255a\u2550\u2550\u2550\u255d \u255a\u2550\u2550\u2550\u2550\u2550\u255d\u255a\u2550\u255d  \u255a\u2550\u255d\n\nUsage: scale-launch [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --help  Show this message and exit.\n\nCommands:\n  batch-jobs  Batch Jobs is a wrapper around batch jobs in Scale Launch\n  bundles     Bundles is a wrapper around model bundles in Scale Launch\n  config      Config is a wrapper around getting and setting your API key and other configuration options\n  endpoints   Endpoints is a wrapper around model endpoints in Scale Launch\n  tasks       Tasks is a wrapper around sending requests to endpoints\n</code></pre>"},{"location":"api/client/","title":"Launch Client","text":""},{"location":"api/client/#launch.client.LaunchClient","title":"LaunchClient","text":"<pre><code>LaunchClient(\napi_key: str,\nendpoint: Optional[str] = None,\nself_hosted: bool = False,\n)\n</code></pre> <p>Scale Launch Python Client.</p> <p>Initializes a Scale Launch Client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>Your Scale API key</p> required <code>endpoint</code> <code>Optional[str]</code> <p>The Scale Launch Endpoint (this should not need to be changed)</p> <code>None</code> <code>self_hosted</code> <code>bool</code> <p>True iff you are connecting to a self-hosted Scale Launch</p> <code>False</code>"},{"location":"api/client/#launch.client.LaunchClient.batch_async_request","title":"batch_async_request","text":"<pre><code>batch_async_request(\n*,\nmodel_bundle: Union[ModelBundle, str],\nurls: List[str] = None,\ninputs: Optional[List[Dict[str, Any]]] = None,\nbatch_url_file_location: Optional[str] = None,\nserialization_format: str = \"JSON\",\nlabels: Optional[Dict[str, str]] = None,\ncpus: Optional[int] = None,\nmemory: Optional[str] = None,\ngpus: Optional[int] = None,\ngpu_type: Optional[str] = None,\nstorage: Optional[str] = None,\nmax_workers: Optional[int] = None,\nper_worker: Optional[int] = None\n) -&gt; Dict[str, Any]\n</code></pre> <p>Sends a batch inference request using a given bundle. Returns a key that can be used to retrieve the results of inference at a later time.</p> <p>Must have exactly one of urls or inputs passed in.</p> <p>Parameters:</p> Name Type Description Default <code>model_bundle</code> <code>Union[ModelBundle, str]</code> <p>The bundle or the name of a the bundle to use for inference.</p> required <code>urls</code> <code>List[str]</code> <p>A list of urls, each pointing to a file containing model input. Must be accessible by Scale Launch, hence urls need to either be public or signedURLs.</p> <code>None</code> <code>inputs</code> <code>Optional[List[Dict[str, Any]]]</code> <p>A list of model inputs, if exists, we will upload the inputs and pass it in to Launch.</p> <code>None</code> <code>batch_url_file_location</code> <code>Optional[str]</code> <p>In self-hosted mode, the input to the batch job will be uploaded to this location if provided. Otherwise, one will be determined from bundle_location_fn()</p> <code>None</code> <code>serialization_format</code> <code>str</code> <p>Serialization format of output, either 'PICKLE' or 'JSON'. 'pickle' corresponds to pickling results + returning</p> <code>'JSON'</code> <code>labels</code> <code>Optional[Dict[str, str]]</code> <p>An optional dictionary of key/value pairs to associate with this endpoint.</p> <code>None</code> <code>cpus</code> <code>Optional[int]</code> <p>Number of cpus each worker should get, e.g. 1, 2, etc. This must be greater than or equal to 1.</p> <code>None</code> <code>memory</code> <code>Optional[str]</code> <p>Amount of memory each worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of memory.</p> <code>None</code> <code>storage</code> <code>Optional[str]</code> <p>Amount of local ephemeral storage each worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of storage.</p> <code>None</code> <code>gpus</code> <code>Optional[int]</code> <p>Number of gpus each worker should get, e.g. 0, 1, etc.</p> <code>None</code> <code>max_workers</code> <code>Optional[int]</code> <p>The maximum number of workers. Must be greater than or equal to 0, and as well as greater than or equal to <code>min_workers</code>.</p> <code>None</code> <code>per_worker</code> <code>Optional[int]</code> <p>The maximum number of concurrent requests that an individual worker can service. Launch automatically scales the number of workers for the endpoint so that each worker is processing <code>per_worker</code> requests:</p> <ul> <li>If the average number of concurrent requests per worker is lower than   <code>per_worker</code>, then the number of workers will be reduced.</li> <li>Otherwise, if the average number of concurrent requests per worker is higher   than <code>per_worker</code>, then the number of workers will be increased to meet the   elevated traffic.</li> </ul> <code>None</code> <code>gpu_type</code> <code>Optional[str]</code> <p>If specifying a non-zero number of gpus, this controls the type of gpu requested. Here are the supported values:</p> <ul> <li><code>nvidia-tesla-t4</code></li> <li><code>nvidia-ampere-a10</code></li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary that contains <code>job_id</code> as a key, and the ID as the value.</p>"},{"location":"api/client/#launch.client.LaunchClient.clone_model_bundle_with_changes","title":"clone_model_bundle_with_changes","text":"<pre><code>clone_model_bundle_with_changes(\nmodel_bundle: Union[ModelBundle, str],\napp_config: Optional[Dict] = None,\n) -&gt; ModelBundle\n</code></pre> Warning <p>This method is deprecated. Use <code>clone_model_bundle_with_changes_v2</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>model_bundle</code> <code>Union[ModelBundle, str]</code> <p>The existing bundle or its ID.</p> required <code>app_config</code> <code>Optional[Dict]</code> <p>The new bundle's app config, if not passed in, the new bundle's <code>app_config</code> will be set to <code>None</code></p> <code>None</code> <p>Returns:</p> Type Description <code>ModelBundle</code> <p>A <code>ModelBundle</code> object</p>"},{"location":"api/client/#launch.client.LaunchClient.clone_model_bundle_with_changes_v2","title":"clone_model_bundle_with_changes_v2","text":"<pre><code>clone_model_bundle_with_changes_v2(\noriginal_model_bundle_id: str,\nnew_app_config: Optional[Dict[str, Any]] = None,\n) -&gt; CreateModelBundleV2Response\n</code></pre> <p>Clone a model bundle with an optional new <code>app_config</code>.</p> <p>Parameters:</p> Name Type Description Default <code>original_model_bundle_id</code> <code>str</code> <p>The ID of the model bundle you want to clone.</p> required <code>new_app_config</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary of new app config values to use for the cloned model.</p> <code>None</code> <p>Returns:</p> Type Description <code>CreateModelBundleV2Response</code> <p>An object containing the following keys:</p> <ul> <li><code>model_bundle_id</code>: The ID of the cloned model bundle.</li> </ul>"},{"location":"api/client/#launch.client.LaunchClient.create_model_bundle","title":"create_model_bundle","text":"<pre><code>create_model_bundle(\nmodel_bundle_name: str,\nenv_params: Dict[str, str],\n*,\nload_predict_fn: Optional[\nCallable[[LaunchModel_T], Callable[[Any], Any]]\n] = None,\npredict_fn_or_cls: Optional[\nCallable[[Any], Any]\n] = None,\nrequirements: Optional[List[str]] = None,\nmodel: Optional[LaunchModel_T] = None,\nload_model_fn: Optional[\nCallable[[], LaunchModel_T]\n] = None,\napp_config: Optional[Union[Dict[str, Any], str]] = None,\nglobals_copy: Optional[Dict[str, Any]] = None,\nrequest_schema: Optional[Type[BaseModel]] = None,\nresponse_schema: Optional[Type[BaseModel]] = None\n) -&gt; ModelBundle\n</code></pre> Warning <p>This method is deprecated. Use <code>create_model_bundle_from_callable_v2</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>model_bundle_name</code> <code>str</code> <p>The name of the model bundle you want to create. The name must be unique across all bundles that you own.</p> required <code>predict_fn_or_cls</code> <code>Optional[Callable[[Any], Any]]</code> <p><code>Function</code> or a <code>Callable</code> class that runs end-to-end (pre/post processing and model inference) on the call. i.e. <code>predict_fn_or_cls(REQUEST) -&gt; RESPONSE</code>.</p> <code>None</code> <code>model</code> <code>Optional[LaunchModel_T]</code> <p>Typically a trained Neural Network, e.g. a Pytorch module.</p> <p>Exactly one of <code>model</code> and <code>load_model_fn</code> must be provided.</p> <code>None</code> <code>load_model_fn</code> <code>Optional[Callable[[], LaunchModel_T]]</code> <p>A function that, when run, loads a model. This function is essentially a deferred wrapper around the <code>model</code> argument.</p> <p>Exactly one of <code>model</code> and <code>load_model_fn</code> must be provided.</p> <code>None</code> <code>load_predict_fn</code> <code>Optional[Callable[[LaunchModel_T], Callable[[Any], Any]]]</code> <p>Function that, when called with a model, returns a function that carries out inference.</p> <p>If <code>model</code> is specified, then this is equivalent to:     <code>load_predict_fn(model, app_config=optional_app_config]) -&gt; predict_fn</code></p> <p>Otherwise, if <code>load_model_fn</code> is specified, then this is equivalent to: <code>load_predict_fn(load_model_fn(), app_config=optional_app_config]) -&gt; predict_fn</code></p> <p>In both cases, <code>predict_fn</code> is then the inference function, i.e.:     <code>predict_fn(REQUEST) -&gt; RESPONSE</code></p> <code>None</code> <code>requirements</code> <code>Optional[List[str]]</code> <p>A list of python package requirements, where each list element is of the form <code>&lt;package_name&gt;==&lt;package_version&gt;</code>, e.g.</p> <p><code>[\"tensorflow==2.3.0\", \"tensorflow-hub==0.11.0\"]</code></p> <p>If you do not pass in a value for <code>requirements</code>, then you must pass in <code>globals()</code> for the <code>globals_copy</code> argument.</p> <code>None</code> <code>app_config</code> <code>Optional[Union[Dict[str, Any], str]]</code> <p>Either a Dictionary that represents a YAML file contents or a local path to a YAML file.</p> <code>None</code> <code>env_params</code> <code>Dict[str, str]</code> <p>A dictionary that dictates environment information e.g. the use of pytorch or tensorflow, which base image tag to use, etc. Specifically, the dictionary should contain the following keys:</p> <ul> <li> <p><code>framework_type</code>: either <code>tensorflow</code> or <code>pytorch</code>. - PyTorch fields: - <code>pytorch_image_tag</code>: An image tag for the <code>pytorch</code> docker base image. The list of tags can be found from https://hub.docker.com/r/pytorch/pytorch/tags. - Example:</p> <p>.. code-block:: python</p> <p>{        \"framework_type\": \"pytorch\",        \"pytorch_image_tag\": \"1.10.0-cuda11.3-cudnn8-runtime\"    }</p> </li> <li> <p>Tensorflow fields:</p> <ul> <li><code>tensorflow_version</code>: Version of tensorflow, e.g. <code>\"2.3.0\"</code>.</li> </ul> </li> </ul> required <code>globals_copy</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of the global symbol table. Normally provided by <code>globals()</code> built-in function.</p> <code>None</code> <code>request_schema</code> <code>Optional[Type[BaseModel]]</code> <p>A pydantic model that represents the request schema for the model bundle. This is used to validate the request body for the model bundle's endpoint.</p> <code>None</code> <code>response_schema</code> <code>Optional[Type[BaseModel]]</code> <p>A pydantic model that represents the request schema for the model bundle. This is used to validate the response for the model bundle's endpoint. Note: If request_schema is specified, then response_schema must also be specified.</p> <code>None</code>"},{"location":"api/client/#launch.client.LaunchClient.create_model_bundle_from_callable_v2","title":"create_model_bundle_from_callable_v2","text":"<pre><code>create_model_bundle_from_callable_v2(\n*,\nmodel_bundle_name: str,\nload_predict_fn: Callable[\n[LaunchModel_T], Callable[[Any], Any]\n],\nload_model_fn: Callable[[], LaunchModel_T],\nrequest_schema: Type[BaseModel],\nresponse_schema: Type[BaseModel],\nrequirements: Optional[List[str]] = None,\npytorch_image_tag: Optional[str] = None,\ntensorflow_version: Optional[str] = None,\ncustom_base_image_repository: Optional[str] = None,\ncustom_base_image_tag: Optional[str] = None,\napp_config: Optional[Union[Dict[str, Any], str]] = None\n) -&gt; CreateModelBundleV2Response\n</code></pre> <p>Uploads and registers a model bundle to Scale Launch.</p> <p>Parameters:</p> Name Type Description Default <code>model_bundle_name</code> <code>str</code> <p>Name of the model bundle.</p> required <code>load_predict_fn</code> <code>Callable[[LaunchModel_T], Callable[[Any], Any]]</code> <p>Function that takes in a model and returns a predict function. When your model bundle is deployed, this predict function will be called as follows: <pre><code>input = {\"input\": \"some input\"} # or whatever your request schema is.\n\ndef load_model_fn():\n    # load model\n    return model\n\ndef load_predict_fn(model, app_config=None):\n    def predict_fn(input):\n        # do pre-processing\n        output = model(input)\n        # do post-processing\n        return output\n    return predict_fn\n\npredict_fn = load_predict_fn(load_model_fn(), app_config=optional_app_config)\nresponse = predict_fn(input)\n</code></pre></p> required <code>load_model_fn</code> <code>Callable[[], LaunchModel_T]</code> <p>A function that, when run, loads a model.</p> required <code>request_schema</code> <code>Type[BaseModel]</code> <p>A pydantic model that represents the request schema for the model bundle. This is used to validate the request body for the model bundle's endpoint.</p> required <code>response_schema</code> <code>Type[BaseModel]</code> <p>A pydantic model that represents the request schema for the model bundle. This is used to validate the response for the model bundle's endpoint.</p> required <code>requirements</code> <code>Optional[List[str]]</code> <p>List of pip requirements.</p> <code>None</code> <code>pytorch_image_tag</code> <code>Optional[str]</code> <p>The image tag for the PyTorch image that will be used to run the bundle. Exactly one of <code>pytorch_image_tag</code>, <code>tensorflow_version</code>, or <code>custom_base_image_repository</code> must be specified.</p> <code>None</code> <code>tensorflow_version</code> <code>Optional[str]</code> <p>The version of TensorFlow that will be used to run the bundle. If not specified, the default version will be used. Exactly one of <code>pytorch_image_tag</code>, <code>tensorflow_version</code>, or <code>custom_base_image_repository</code> must be specified.</p> <code>None</code> <code>custom_base_image_repository</code> <code>Optional[str]</code> <p>The repository for a custom base image that will be used to run the bundle. If not specified, the default base image will be used. Exactly one of <code>pytorch_image_tag</code>, <code>tensorflow_version</code>, or <code>custom_base_image_repository</code> must be specified.</p> <code>None</code> <code>custom_base_image_tag</code> <code>Optional[str]</code> <p>The tag for a custom base image that will be used to run the bundle. Must be specified if <code>custom_base_image_repository</code> is specified.</p> <code>None</code> <code>app_config</code> <code>Optional[Union[Dict[str, Any], str]]</code> <p>An optional dictionary of configuration values that will be passed to the bundle when it is run. These values can be accessed by the bundle via the <code>app_config</code> global variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>CreateModelBundleV2Response</code> <p>An object containing the following keys:</p> <ul> <li><code>model_bundle_id</code>: The ID of the created model bundle.</li> </ul>"},{"location":"api/client/#launch.client.LaunchClient.create_model_bundle_from_dirs","title":"create_model_bundle_from_dirs","text":"<pre><code>create_model_bundle_from_dirs(\n*,\nmodel_bundle_name: str,\nbase_paths: List[str],\nrequirements_path: str,\nenv_params: Dict[str, str],\nload_predict_fn_module_path: str,\nload_model_fn_module_path: str,\napp_config: Optional[Union[Dict[str, Any], str]] = None,\nrequest_schema: Optional[Type[BaseModel]] = None,\nresponse_schema: Optional[Type[BaseModel]] = None\n) -&gt; ModelBundle\n</code></pre> Warning <p>This method is deprecated. Use <code>create_model_bundle_from_dirs_v2</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>model_bundle_name</code> <code>str</code> <p>The name of the model bundle you want to create. The name must be unique across all bundles that you own.</p> required <code>base_paths</code> <code>List[str]</code> <p>The paths on the local filesystem where the bundle code lives.</p> required <code>requirements_path</code> <code>str</code> <p>A path on the local filesystem where a <code>requirements.txt</code> file lives.</p> required <code>env_params</code> <code>Dict[str, str]</code> <p>A dictionary that dictates environment information e.g. the use of pytorch or tensorflow, which base image tag to use, etc. Specifically, the dictionary should contain the following keys:</p> <ul> <li><code>framework_type</code>: either <code>tensorflow</code> or <code>pytorch</code>.</li> <li>PyTorch fields:<ul> <li><code>pytorch_image_tag</code>: An image tag for the <code>pytorch</code> docker base image. The     list of tags can be found from https://hub.docker.com/r/pytorch/pytorch/tags</li> </ul> </li> </ul> <p>Example:    <pre><code>{\n\"framework_type\": \"pytorch\",\n\"pytorch_image_tag\": \"1.10.0-cuda11.3-cudnn8-runtime\",\n}\n</code></pre></p> required <code>load_predict_fn_module_path</code> <code>str</code> <p>A python module path for a function that, when called with the output of load_model_fn_module_path, returns a function that carries out inference.</p> required <code>load_model_fn_module_path</code> <code>str</code> <p>A python module path for a function that returns a model. The output feeds into the function located at load_predict_fn_module_path.</p> required <code>app_config</code> <code>Optional[Union[Dict[str, Any], str]]</code> <p>Either a Dictionary that represents a YAML file contents or a local path to a YAML file.</p> <code>None</code> <code>request_schema</code> <code>Optional[Type[BaseModel]]</code> <p>A pydantic model that represents the request schema for the model bundle. This is used to validate the request body for the model bundle's endpoint.</p> <code>None</code> <code>response_schema</code> <code>Optional[Type[BaseModel]]</code> <p>A pydantic model that represents the request schema for the model bundle. This is used to validate the response for the model bundle's endpoint. Note: If request_schema is specified, then response_schema must also be specified.</p> <code>None</code>"},{"location":"api/client/#launch.client.LaunchClient.create_model_bundle_from_dirs_v2","title":"create_model_bundle_from_dirs_v2","text":"<pre><code>create_model_bundle_from_dirs_v2(\n*,\nmodel_bundle_name: str,\nbase_paths: List[str],\nload_predict_fn_module_path: str,\nload_model_fn_module_path: str,\nrequest_schema: Type[BaseModel],\nresponse_schema: Type[BaseModel],\nrequirements_path: str,\npytorch_image_tag: Optional[str] = None,\ntensorflow_version: Optional[str] = None,\ncustom_base_image_repository: Optional[str] = None,\ncustom_base_image_tag: Optional[str] = None,\napp_config: Optional[Dict[str, Any]] = None\n) -&gt; CreateModelBundleV2Response\n</code></pre> <p>Packages up code from one or more local filesystem folders and uploads them as a bundle to Scale Launch. In this mode, a bundle is just local code instead of a serialized object.</p> <p>For example, if you have a directory structure like so, and your current working directory is <code>my_root</code>:</p> <pre><code>   my_root/\n       my_module1/\n           __init__.py\n           ...files and directories\n           my_inference_file.py\n       my_module2/\n           __init__.py\n           ...files and directories\n</code></pre> <p>then calling <code>create_model_bundle_from_dirs_v2</code> with <code>base_paths=[\"my_module1\", \"my_module2\"]</code> essentially creates a zip file without the root directory, e.g.:</p> <pre><code>   my_module1/\n       __init__.py\n       ...files and directories\n       my_inference_file.py\n   my_module2/\n       __init__.py\n       ...files and directories\n</code></pre> <p>and these contents will be unzipped relative to the server side application root. Bear these points in mind when referencing Python module paths for this bundle. For instance, if <code>my_inference_file.py</code> has <code>def f(...)</code> as the desired inference loading function, then the <code>load_predict_fn_module_path</code> argument should be <code>my_module1.my_inference_file.f</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_bundle_name</code> <code>str</code> <p>The name of the model bundle you want to create.</p> required <code>base_paths</code> <code>List[str]</code> <p>A list of paths to directories that will be zipped up and uploaded as a bundle. Each path must be relative to the current working directory.</p> required <code>load_predict_fn_module_path</code> <code>str</code> <p>The Python module path to the function that will be used to load the model for inference. This function should take in a path to a model directory, and return a model object. The model object should be pickleable.</p> required <code>load_model_fn_module_path</code> <code>str</code> <p>The Python module path to the function that will be used to load the model for training. This function should take in a path to a model directory, and return a model object. The model object should be pickleable.</p> required <code>request_schema</code> <code>Type[BaseModel]</code> <p>A Pydantic model that defines the request schema for the bundle.</p> required <code>response_schema</code> <code>Type[BaseModel]</code> <p>A Pydantic model that defines the response schema for the bundle.</p> required <code>requirements_path</code> <code>str</code> <p>Path to a requirements.txt file that will be used to install dependencies for the bundle. This file must be relative to the current working directory.</p> required <code>pytorch_image_tag</code> <code>Optional[str]</code> <p>The image tag for the PyTorch image that will be used to run the bundle. Exactly one of <code>pytorch_image_tag</code>, <code>tensorflow_version</code>, or <code>custom_base_image_repository</code> must be specified.</p> <code>None</code> <code>tensorflow_version</code> <code>Optional[str]</code> <p>The version of TensorFlow that will be used to run the bundle. If not specified, the default version will be used. Exactly one of <code>pytorch_image_tag</code>, <code>tensorflow_version</code>, or <code>custom_base_image_repository</code> must be specified.</p> <code>None</code> <code>custom_base_image_repository</code> <code>Optional[str]</code> <p>The repository for a custom base image that will be used to run the bundle. If not specified, the default base image will be used. Exactly one of <code>pytorch_image_tag</code>, <code>tensorflow_version</code>, or <code>custom_base_image_repository</code> must be specified.</p> <code>None</code> <code>custom_base_image_tag</code> <code>Optional[str]</code> <p>The tag for a custom base image that will be used to run the bundle. Must be specified if <code>custom_base_image_repository</code> is specified.</p> <code>None</code> <code>app_config</code> <code>Optional[Dict[str, Any]]</code> <p>An optional dictionary of configuration values that will be passed to the bundle when it is run. These values can be accessed by the bundle via the <code>app_config</code> global variable.</p> <code>None</code> <p>Returns:</p> Type Description <code>CreateModelBundleV2Response</code> <p>An object containing the following keys:</p> <ul> <li><code>model_bundle_id</code>: The ID of the created model bundle.</li> </ul>"},{"location":"api/client/#launch.client.LaunchClient.create_model_bundle_from_runnable_image_v2","title":"create_model_bundle_from_runnable_image_v2","text":"<pre><code>create_model_bundle_from_runnable_image_v2(\n*,\nmodel_bundle_name: str,\nrequest_schema: Type[BaseModel],\nresponse_schema: Type[BaseModel],\nrepository: str,\ntag: str,\ncommand: List[str],\nenv: Dict[str, str]\n) -&gt; CreateModelBundleV2Response\n</code></pre> <p>Create a model bundle from a runnable image. The specified <code>command</code> must start a process that will listen for requests on port 5005 using HTTP.</p> <p>Parameters:</p> Name Type Description Default <code>model_bundle_name</code> <code>str</code> <p>The name of the model bundle you want to create.</p> required <code>request_schema</code> <code>Type[BaseModel]</code> <p>A Pydantic model that defines the request schema for the bundle.</p> required <code>response_schema</code> <code>Type[BaseModel]</code> <p>A Pydantic model that defines the response schema for the bundle.</p> required <code>repository</code> <code>str</code> <p>The name of the Docker repository for the runnable image.</p> required <code>tag</code> <code>str</code> <p>The tag for the runnable image.</p> required <code>command</code> <code>List[str]</code> <p>The command that will be used to start the process that listens for requests.</p> required <code>env</code> <code>Dict[str, str]</code> <p>A dictionary of environment variables that will be passed to the bundle when it is run.</p> required <p>Returns:</p> Type Description <code>CreateModelBundleV2Response</code> <p>An object containing the following keys:</p> <ul> <li><code>model_bundle_id</code>: The ID of the created model bundle.</li> </ul>"},{"location":"api/client/#launch.client.LaunchClient.create_model_endpoint","title":"create_model_endpoint","text":"<pre><code>create_model_endpoint(\n*,\nendpoint_name: str,\nmodel_bundle: Union[ModelBundle, str],\ncpus: int = 3,\nmemory: str = \"8Gi\",\nstorage: Optional[str] = None,\ngpus: int = 0,\nmin_workers: int = 1,\nmax_workers: int = 1,\nper_worker: int = 10,\ngpu_type: Optional[str] = None,\nendpoint_type: str = \"sync\",\nhigh_priority: Optional[bool] = False,\npost_inference_hooks: Optional[\nList[PostInferenceHooks]\n] = None,\ndefault_callback_url: Optional[str] = None,\ndefault_callback_auth_kind: Optional[\nLiteral[basic, mtls]\n] = None,\ndefault_callback_auth_username: Optional[str] = None,\ndefault_callback_auth_password: Optional[str] = None,\ndefault_callback_auth_cert: Optional[str] = None,\ndefault_callback_auth_key: Optional[str] = None,\nupdate_if_exists: bool = False,\nlabels: Optional[Dict[str, str]] = None\n) -&gt; Optional[Endpoint]\n</code></pre> <p>Creates and registers a model endpoint in Scale Launch. The returned object is an instance of type <code>Endpoint</code>, which is a base class of either <code>SyncEndpoint</code> or <code>AsyncEndpoint</code>. This is the object to which you sent inference requests.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>The name of the model endpoint you want to create. The name must be unique across all endpoints that you own.</p> required <code>model_bundle</code> <code>Union[ModelBundle, str]</code> <p>The <code>ModelBundle</code> that the endpoint should serve.</p> required <code>cpus</code> <code>int</code> <p>Number of cpus each worker should get, e.g. 1, 2, etc. This must be greater than or equal to 1.</p> <code>3</code> <code>memory</code> <code>str</code> <p>Amount of memory each worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of memory.</p> <code>'8Gi'</code> <code>storage</code> <code>Optional[str]</code> <p>Amount of local ephemeral storage each worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of storage.</p> <code>None</code> <code>gpus</code> <code>int</code> <p>Number of gpus each worker should get, e.g. 0, 1, etc.</p> <code>0</code> <code>min_workers</code> <code>int</code> <p>The minimum number of workers. Must be greater than or equal to 0. This should be determined by computing the minimum throughput of your workload and dividing it by the throughput of a single worker. This field must be at least <code>1</code> for synchronous endpoints.</p> <code>1</code> <code>max_workers</code> <code>int</code> <p>The maximum number of workers. Must be greater than or equal to 0, and as well as greater than or equal to <code>min_workers</code>. This should be determined by computing the maximum throughput of your workload and dividing it by the throughput of a single worker.</p> <code>1</code> <code>per_worker</code> <code>int</code> <p>The maximum number of concurrent requests that an individual worker can service. Launch automatically scales the number of workers for the endpoint so that each worker is processing <code>per_worker</code> requests, subject to the limits defined by <code>min_workers</code> and <code>max_workers</code>.</p> <ul> <li>If the average number of concurrent requests per worker is lower than <code>per_worker</code>, then the number of workers will be reduced. - Otherwise, if the average number of concurrent requests per worker is higher than <code>per_worker</code>, then the number of workers will be increased to meet the elevated traffic.</li> </ul> <p>Here is our recommendation for computing <code>per_worker</code>:</p> <ol> <li>Compute <code>min_workers</code> and <code>max_workers</code> per your minimum and maximum throughput requirements. 2. Determine a value for the maximum number of concurrent requests in the workload. Divide this number by <code>max_workers</code>. Doing this ensures that the number of workers will \"climb\" to <code>max_workers</code>.</li> </ol> <code>10</code> <code>gpu_type</code> <code>Optional[str]</code> <p>If specifying a non-zero number of gpus, this controls the type of gpu requested. Here are the supported values:</p> <ul> <li><code>nvidia-tesla-t4</code></li> <li><code>nvidia-ampere-a10</code></li> </ul> <code>None</code> <code>endpoint_type</code> <code>str</code> <p>Either <code>\"sync\"</code> or <code>\"async\"</code>.</p> <code>'sync'</code> <code>high_priority</code> <code>Optional[bool]</code> <p>Either <code>True</code> or <code>False</code>. Enabling this will allow the created endpoint to leverage the shared pool of prewarmed nodes for faster spinup time.</p> <code>False</code> <code>post_inference_hooks</code> <code>Optional[List[PostInferenceHooks]]</code> <p>List of hooks to trigger after inference tasks are served.</p> <code>None</code> <code>default_callback_url</code> <code>Optional[str]</code> <p>The default callback url to use for async endpoints. This can be overridden in the task parameters for each individual task. post_inference_hooks must contain \"callback\" for the callback to be triggered.</p> <code>None</code> <code>default_callback_auth_kind</code> <code>Optional[Literal[basic, mtls]]</code> <p>The default callback auth kind to use for async endpoints. Either \"basic\" or \"mtls\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>default_callback_auth_username</code> <code>Optional[str]</code> <p>The default callback auth username to use. This only applies if default_callback_auth_kind is \"basic\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>default_callback_auth_password</code> <code>Optional[str]</code> <p>The default callback auth password to use. This only applies if default_callback_auth_kind is \"basic\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>default_callback_auth_cert</code> <code>Optional[str]</code> <p>The default callback auth cert to use. This only applies if default_callback_auth_kind is \"mtls\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>default_callback_auth_key</code> <code>Optional[str]</code> <p>The default callback auth key to use. This only applies if default_callback_auth_kind is \"mtls\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>update_if_exists</code> <code>bool</code> <p>If <code>True</code>, will attempt to update the endpoint if it exists. Otherwise, will unconditionally try to create a new endpoint. Note that endpoint names for a given user must be unique, so attempting to call this function with <code>update_if_exists=False</code> for an existing endpoint will raise an error.</p> <code>False</code> <code>labels</code> <code>Optional[Dict[str, str]]</code> <p>An optional dictionary of key/value pairs to associate with this endpoint.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Endpoint]</code> <p>A Endpoint object that can be used to make requests to the endpoint.</p>"},{"location":"api/client/#launch.client.LaunchClient.delete_model_endpoint","title":"delete_model_endpoint","text":"<pre><code>delete_model_endpoint(\nmodel_endpoint: Union[ModelEndpoint, str]\n)\n</code></pre> <p>Deletes a model endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>model_endpoint</code> <code>Union[ModelEndpoint, str]</code> <p>A <code>ModelEndpoint</code> object.</p> required"},{"location":"api/client/#launch.client.LaunchClient.edit_model_endpoint","title":"edit_model_endpoint","text":"<pre><code>edit_model_endpoint(\n*,\nmodel_endpoint: Union[ModelEndpoint, str],\nmodel_bundle: Optional[Union[ModelBundle, str]] = None,\ncpus: Optional[float] = None,\nmemory: Optional[str] = None,\nstorage: Optional[str] = None,\ngpus: Optional[int] = None,\nmin_workers: Optional[int] = None,\nmax_workers: Optional[int] = None,\nper_worker: Optional[int] = None,\ngpu_type: Optional[str] = None,\nhigh_priority: Optional[bool] = None,\npost_inference_hooks: Optional[\nList[PostInferenceHooks]\n] = None,\ndefault_callback_url: Optional[str] = None,\ndefault_callback_auth_kind: Optional[\nLiteral[basic, mtls]\n] = None,\ndefault_callback_auth_username: Optional[str] = None,\ndefault_callback_auth_password: Optional[str] = None,\ndefault_callback_auth_cert: Optional[str] = None,\ndefault_callback_auth_key: Optional[str] = None\n) -&gt; None\n</code></pre> <p>Edits an existing model endpoint. Here are the fields that cannot be edited on an existing endpoint:</p> <ul> <li>The endpoint's name. - The endpoint's type (i.e. you cannot go from a <code>SyncEnpdoint</code> to an <code>AsyncEndpoint</code> or vice versa.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model_endpoint</code> <code>Union[ModelEndpoint, str]</code> <p>The model endpoint (or its name) you want to edit. The name must be unique across all endpoints that you own.</p> required <code>model_bundle</code> <code>Optional[Union[ModelBundle, str]]</code> <p>The <code>ModelBundle</code> that the endpoint should serve.</p> <code>None</code> <code>cpus</code> <code>Optional[float]</code> <p>Number of cpus each worker should get, e.g. 1, 2, etc. This must be greater than or equal to 1.</p> <code>None</code> <code>memory</code> <code>Optional[str]</code> <p>Amount of memory each worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of memory.</p> <code>None</code> <code>storage</code> <code>Optional[str]</code> <p>Amount of local ephemeral storage each worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of storage.</p> <code>None</code> <code>gpus</code> <code>Optional[int]</code> <p>Number of gpus each worker should get, e.g. 0, 1, etc.</p> <code>None</code> <code>min_workers</code> <code>Optional[int]</code> <p>The minimum number of workers. Must be greater than or equal to 0.</p> <code>None</code> <code>max_workers</code> <code>Optional[int]</code> <p>The maximum number of workers. Must be greater than or equal to 0, and as well as greater than or equal to <code>min_workers</code>.</p> <code>None</code> <code>per_worker</code> <code>Optional[int]</code> <p>The maximum number of concurrent requests that an individual worker can service. Launch automatically scales the number of workers for the endpoint so that each worker is processing <code>per_worker</code> requests:</p> <ul> <li>If the average number of concurrent requests per worker is lower than <code>per_worker</code>, then the number of workers will be reduced. - Otherwise, if the average number of concurrent requests per worker is higher than <code>per_worker</code>, then the number of workers will be increased to meet the elevated traffic.</li> </ul> <code>None</code> <code>gpu_type</code> <code>Optional[str]</code> <p>If specifying a non-zero number of gpus, this controls the type of gpu requested. Here are the supported values:</p> <ul> <li><code>nvidia-tesla-t4</code></li> <li><code>nvidia-ampere-a10</code></li> </ul> <code>None</code> <code>high_priority</code> <code>Optional[bool]</code> <p>Either <code>True</code> or <code>False</code>. Enabling this will allow the created endpoint to leverage the shared pool of prewarmed nodes for faster spinup time.</p> <code>None</code> <code>post_inference_hooks</code> <code>Optional[List[PostInferenceHooks]]</code> <p>List of hooks to trigger after inference tasks are served.</p> <code>None</code> <code>default_callback_url</code> <code>Optional[str]</code> <p>The default callback url to use for async endpoints. This can be overridden in the task parameters for each individual task. post_inference_hooks must contain \"callback\" for the callback to be triggered.</p> <code>None</code> <code>default_callback_auth_kind</code> <code>Optional[Literal[basic, mtls]]</code> <p>The default callback auth kind to use for async endpoints. Either \"basic\" or \"mtls\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>default_callback_auth_username</code> <code>Optional[str]</code> <p>The default callback auth username to use. This only applies if default_callback_auth_kind is \"basic\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>default_callback_auth_password</code> <code>Optional[str]</code> <p>The default callback auth password to use. This only applies if default_callback_auth_kind is \"basic\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>default_callback_auth_cert</code> <code>Optional[str]</code> <p>The default callback auth cert to use. This only applies if default_callback_auth_kind is \"mtls\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>default_callback_auth_key</code> <code>Optional[str]</code> <p>The default callback auth key to use. This only applies if default_callback_auth_kind is \"mtls\". This can be overridden in the task parameters for each individual task.</p> <code>None</code>"},{"location":"api/client/#launch.client.LaunchClient.get_batch_async_response","title":"get_batch_async_response","text":"<pre><code>get_batch_async_response(\nbatch_job_id: str,\n) -&gt; Dict[str, Any]\n</code></pre> <p>Gets inference results from a previously created batch job.</p> <p>Parameters:</p> Name Type Description Default <code>batch_job_id</code> <code>str</code> <p>An id representing the batch task job. This id is the in the response from calling <code>batch_async_request</code>.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>A dictionary that contains the following fields:</p> <code>Dict[str, Any]</code> <ul> <li><code>status</code>: The status of the job.</li> </ul> <code>Dict[str, Any]</code> <ul> <li><code>result</code>: The url where the result is stored.</li> </ul> <code>Dict[str, Any]</code> <ul> <li><code>duration</code>: A string representation of how long the job took to finish     or how long it has been running, for a job current in progress.</li> </ul> <code>Dict[str, Any]</code> <ul> <li><code>num_tasks_pending</code>: The number of tasks that are still pending.</li> </ul> <code>Dict[str, Any]</code> <ul> <li><code>num_tasks_completed</code>: The number of tasks that have completed.</li> </ul>"},{"location":"api/client/#launch.client.LaunchClient.get_latest_model_bundle_v2","title":"get_latest_model_bundle_v2","text":"<pre><code>get_latest_model_bundle_v2(\nmodel_bundle_name: str,\n) -&gt; ModelBundleV2Response\n</code></pre> <p>Get the latest version of a model bundle.</p> <p>Parameters:</p> Name Type Description Default <code>model_bundle_name</code> <code>str</code> <p>The name of the model bundle you want to get.</p> required <p>Returns:</p> Type Description <code>ModelBundleV2Response</code> <p>An object containing the following keys:</p> <ul> <li><code>id</code>: The ID of the model bundle.</li> <li><code>name</code>: The name of the model bundle.</li> <li><code>schema_location</code>: The location of the schema for the model bundle.</li> <li><code>flavor</code>: The flavor of the model bundle. Either <code>RunnableImage</code>,     <code>CloudpickleArtifact</code>, or <code>ZipArtifact</code>.</li> <li><code>created_at</code>: The time the model bundle was created.</li> <li><code>metadata</code>: A dictionary of metadata associated with the model bundle.</li> <li><code>model_artifact_ids</code>: A list of IDs of model artifacts associated with the     bundle.</li> </ul>"},{"location":"api/client/#launch.client.LaunchClient.get_model_bundle","title":"get_model_bundle","text":"<pre><code>get_model_bundle(\nmodel_bundle: Union[ModelBundle, str]\n) -&gt; ModelBundle\n</code></pre> <p>Returns a model bundle specified by <code>bundle_name</code> that the user owns.</p> <p>Parameters:</p> Name Type Description Default <code>model_bundle</code> <code>Union[ModelBundle, str]</code> <p>The bundle or its name.</p> required <p>Returns:</p> Type Description <code>ModelBundle</code> <p>A <code>ModelBundle</code> object</p>"},{"location":"api/client/#launch.client.LaunchClient.get_model_bundle_v2","title":"get_model_bundle_v2","text":"<pre><code>get_model_bundle_v2(\nmodel_bundle_id: str,\n) -&gt; ModelBundleV2Response\n</code></pre> <p>Get a model bundle.</p> <p>Parameters:</p> Name Type Description Default <code>model_bundle_id</code> <code>str</code> <p>The ID of the model bundle you want to get.</p> required <p>Returns:</p> Type Description <code>ModelBundleV2Response</code> <p>An object containing the following fields:</p> <ul> <li><code>id</code>: The ID of the model bundle.</li> <li><code>name</code>: The name of the model bundle.</li> <li><code>flavor</code>: The flavor of the model bundle. Either <code>RunnableImage</code>,     <code>CloudpickleArtifact</code>, or <code>ZipArtifact</code>.</li> <li><code>created_at</code>: The time the model bundle was created.</li> <li><code>metadata</code>: A dictionary of metadata associated with the model bundle.</li> <li><code>model_artifact_ids</code>: A list of IDs of model artifacts associated with the     bundle.</li> </ul>"},{"location":"api/client/#launch.client.LaunchClient.get_model_endpoint","title":"get_model_endpoint","text":"<pre><code>get_model_endpoint(\nendpoint_name: str,\n) -&gt; Optional[Union[AsyncEndpoint, SyncEndpoint]]\n</code></pre> <p>Gets a model endpoint associated with a name.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>The name of the endpoint to retrieve.</p> required"},{"location":"api/client/#launch.client.LaunchClient.list_model_bundles","title":"list_model_bundles","text":"<pre><code>list_model_bundles() -&gt; List[ModelBundle]\n</code></pre> <p>Returns a list of model bundles that the user owns.</p> <p>Returns:</p> Type Description <code>List[ModelBundle]</code> <p>A list of ModelBundle objects</p>"},{"location":"api/client/#launch.client.LaunchClient.list_model_bundles_v2","title":"list_model_bundles_v2","text":"<pre><code>list_model_bundles_v2() -&gt; ListModelBundlesV2Response\n</code></pre> <p>List all model bundles.</p> <p>Returns:</p> Type Description <code>ListModelBundlesV2Response</code> <p>An object containing the following keys:</p> <ul> <li><code>model_bundles</code>: A list of model bundles. Each model bundle is an object.</li> </ul>"},{"location":"api/client/#launch.client.LaunchClient.list_model_endpoints","title":"list_model_endpoints","text":"<pre><code>list_model_endpoints() -&gt; List[Endpoint]\n</code></pre> <p>Lists all model endpoints that the user owns.</p> <p>Returns:</p> Type Description <code>List[Endpoint]</code> <p>A list of <code>ModelEndpoint</code> objects.</p>"},{"location":"api/client/#launch.client.LaunchClient.read_endpoint_creation_logs","title":"read_endpoint_creation_logs","text":"<pre><code>read_endpoint_creation_logs(\nmodel_endpoint: Union[ModelEndpoint, str]\n)\n</code></pre> <p>Retrieves the logs for the creation of the endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>model_endpoint</code> <code>Union[ModelEndpoint, str]</code> <p>The endpoint or its name.</p> required"},{"location":"api/client/#launch.client.LaunchClient.register_batch_csv_location_fn","title":"register_batch_csv_location_fn","text":"<pre><code>register_batch_csv_location_fn(\nbatch_csv_location_fn: Callable[[], str]\n)\n</code></pre> <p>For self-hosted mode only. Registers a function that gives a location for batch CSV inputs. Should give different locations each time. This function is called as batch_csv_location_fn(), and should return a batch_csv_url that upload_batch_csv_fn can take.</p> <p>Strictly, batch_csv_location_fn() does not need to return a str. The only requirement is that if batch_csv_location_fn returns a value of type T, then upload_batch_csv_fn() takes in an object of type T as its second argument (i.e. batch_csv_url).</p> <p>Parameters:</p> Name Type Description Default <code>batch_csv_location_fn</code> <code>Callable[[], str]</code> <p>Function that generates batch_csv_urls for upload_batch_csv_fn.</p> required"},{"location":"api/client/#launch.client.LaunchClient.register_bundle_location_fn","title":"register_bundle_location_fn","text":"<pre><code>register_bundle_location_fn(\nbundle_location_fn: Callable[[], str]\n)\n</code></pre> <p>For self-hosted mode only. Registers a function that gives a location for a model bundle. Should give different locations each time. This function is called as <code>bundle_location_fn()</code>, and should return a <code>bundle_url</code> that <code>register_upload_bundle_fn</code> can take.</p> <p>Strictly, <code>bundle_location_fn()</code> does not need to return a <code>str</code>. The only requirement is that if <code>bundle_location_fn</code> returns a value of type <code>T</code>, then <code>upload_bundle_fn()</code> takes in an object of type T as its second argument (i.e. bundle_url).</p> <p>Parameters:</p> Name Type Description Default <code>bundle_location_fn</code> <code>Callable[[], str]</code> <p>Function that generates bundle_urls for upload_bundle_fn.</p> required"},{"location":"api/client/#launch.client.LaunchClient.register_upload_batch_csv_fn","title":"register_upload_batch_csv_fn","text":"<pre><code>register_upload_batch_csv_fn(\nupload_batch_csv_fn: Callable[[str, str], None]\n)\n</code></pre> <p>For self-hosted mode only. Registers a function that handles batch text upload. This function is called as</p> <pre><code>upload_batch_csv_fn(csv_text, csv_url)\n</code></pre> <p>This function should directly write the contents of <code>csv_text</code> as a text string into <code>csv_url</code>.</p> <p>Parameters:</p> Name Type Description Default <code>upload_batch_csv_fn</code> <code>Callable[[str, str], None]</code> <p>Function that takes in a csv text (string type), and uploads that bundle to an appropriate location. Only needed for self-hosted mode.</p> required"},{"location":"api/client/#launch.client.LaunchClient.register_upload_bundle_fn","title":"register_upload_bundle_fn","text":"<pre><code>register_upload_bundle_fn(\nupload_bundle_fn: Callable[[str, str], None]\n)\n</code></pre> <p>For self-hosted mode only. Registers a function that handles model bundle upload. This function is called as</p> <pre><code>upload_bundle_fn(serialized_bundle, bundle_url)\n</code></pre> <p>This function should directly write the contents of <code>serialized_bundle</code> as a binary string into <code>bundle_url</code>.</p> <p>See <code>register_bundle_location_fn</code> for more notes on the signature of <code>upload_bundle_fn</code></p> <p>Parameters:</p> Name Type Description Default <code>upload_bundle_fn</code> <code>Callable[[str, str], None]</code> <p>Function that takes in a serialized bundle (bytes type), and uploads that bundle to an appropriate location. Only needed for self-hosted mode.</p> required"},{"location":"api/endpoint_predictions/","title":"Endpoint Predictions","text":""},{"location":"api/endpoint_predictions/#launch.model_endpoint.EndpointRequest","title":"EndpointRequest","text":"<pre><code>EndpointRequest(\nurl: Optional[str] = None,\nargs: Optional[Dict] = None,\ncallback_url: Optional[str] = None,\ncallback_auth_kind: Optional[\nLiteral[basic, mtls]\n] = None,\ncallback_auth_username: Optional[str] = None,\ncallback_auth_password: Optional[str] = None,\ncallback_auth_cert: Optional[str] = None,\ncallback_auth_key: Optional[str] = None,\nreturn_pickled: Optional[bool] = False,\nrequest_id: Optional[str] = None,\n)\n</code></pre> <p>Represents a single request to either a <code>SyncEndpoint</code> or <code>AsyncEndpoint</code>.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Optional[str]</code> <p>A url to some file that can be read in to a ModelBundle's predict function. Can be an image, raw text, etc. Note: the contents of the file located at <code>url</code> are opened as a sequence of <code>bytes</code> and passed to the predict function. If you instead want to pass the url itself as an input to the predict function, see <code>args</code>.</p> <p>Exactly one of <code>url</code> and <code>args</code> must be specified.</p> <code>None</code> <code>args</code> <code>Optional[Dict]</code> <p>A Dictionary with arguments to a ModelBundle's predict function. If the predict function has signature <code>predict_fn(foo, bar)</code>, then the keys in the dictionary should be <code>\"foo\"</code> and <code>\"bar\"</code>. Values must be native Python objects.</p> <p>Exactly one of <code>url</code> and <code>args</code> must be specified.</p> <code>None</code> <code>return_pickled</code> <code>Optional[bool]</code> <p>Whether the output should be a pickled python object, or directly returned serialized json.</p> <code>False</code> <code>callback_url</code> <code>Optional[str]</code> <p>The callback url to use for this task. If None, then the default_callback_url of the endpoint is used. The endpoint must specify \"callback\" as a post-inference hook for the callback to be triggered.</p> <code>None</code> <code>callback_auth_kind</code> <code>Optional[Literal[basic, mtls]]</code> <p>The default callback auth kind to use for async endpoints. Either \"basic\" or \"mtls\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>callback_auth_username</code> <code>Optional[str]</code> <p>The default callback auth username to use. This only applies if callback_auth_kind is \"basic\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>callback_auth_password</code> <code>Optional[str]</code> <p>The default callback auth password to use. This only applies if callback_auth_kind is \"basic\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>callback_auth_cert</code> <code>Optional[str]</code> <p>The default callback auth cert to use. This only applies if callback_auth_kind is \"mtls\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>callback_auth_key</code> <code>Optional[str]</code> <p>The default callback auth key to use. This only applies if callback_auth_kind is \"mtls\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>request_id</code> <code>Optional[str]</code> <p>(deprecated) A user-specifiable id for requests. Should be unique among EndpointRequests made in the same batch call. If one isn't provided the client will generate its own.</p> <code>None</code>"},{"location":"api/endpoint_predictions/#launch.model_endpoint.EndpointResponse","title":"EndpointResponse","text":"<pre><code>EndpointResponse(\nclient,\nstatus: str,\nresult_url: Optional[str] = None,\nresult: Optional[str] = None,\ntraceback: Optional[str] = None,\n)\n</code></pre> <p>Represents a response received from a Endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>An instance of <code>LaunchClient</code>.</p> required <code>status</code> <code>str</code> <p>A string representing the status of the request, i.e. <code>SUCCESS</code>, <code>FAILURE</code>, or <code>PENDING</code></p> required <code>result_url</code> <code>Optional[str]</code> <p>A string that is a url containing the pickled python object from the Endpoint's predict function.</p> <p>Exactly one of <code>result_url</code> or <code>result</code> will be populated, depending on the value of <code>return_pickled</code> in the request.</p> <code>None</code> <code>result</code> <code>Optional[str]</code> <p>A string that is the serialized return value (in json form) of the Endpoint's predict function. Specifically, one can <code>json.loads()</code> the value of result to get the original python object back.</p> <p>Exactly one of <code>result_url</code> or <code>result</code> will be populated, depending on the value of <code>return_pickled</code> in the request.</p> <code>None</code> <code>traceback</code> <code>Optional[str]</code> <p>The stack trace if the inference endpoint raised an error. Can be used for debugging</p> <code>None</code>"},{"location":"api/endpoint_predictions/#launch.model_endpoint.EndpointResponseFuture","title":"EndpointResponseFuture","text":"<pre><code>EndpointResponseFuture(\nclient, endpoint_name: str, async_task_id: str\n)\n</code></pre> <p>Represents a future response from an Endpoint. Specifically, when the <code>EndpointResponseFuture</code> is ready, then its <code>get</code> method will return an actual instance of <code>EndpointResponse</code>.</p> <p>This object should not be directly instantiated by the user.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>An instance of <code>LaunchClient</code>.</p> required <code>endpoint_name</code> <code>str</code> <p>The name of the endpoint.</p> required <code>async_task_id</code> <code>str</code> <p>An async task id.</p> required"},{"location":"api/endpoint_predictions/#launch.model_endpoint.EndpointResponseFuture.get","title":"get","text":"<pre><code>get(timeout: Optional[float] = None) -&gt; EndpointResponse\n</code></pre> <p>Retrieves the <code>EndpointResponse</code> for the prediction request after it completes. This method blocks.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>The maximum number of seconds to wait for the response. If None, then the method will block indefinitely until the response is ready.</p> <code>None</code>"},{"location":"api/hooks/","title":"Hooks","text":""},{"location":"api/hooks/#launch.hooks.PostInferenceHooks","title":"PostInferenceHooks","text":"<p>         Bases: <code>str</code>, <code>Enum</code></p> <p>Post-inference hooks are functions that are called after inference is complete.</p> <p>Attributes:</p> Name Type Description <code>CALLBACK</code> <code>str</code> <p>The callback hook is called with the inference response and the task ID.</p>"},{"location":"api/model_bundles/","title":"Model Bundles","text":""},{"location":"api/model_bundles/#launch.model_bundle.CloudpickleArtifactFlavor","title":"CloudpickleArtifactFlavor","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/model_bundles/#launch.model_bundle.CloudpickleArtifactFlavor.app_config","title":"app_config  <code>class-attribute</code>","text":"<pre><code>app_config: Optional[Dict[str, Any]]\n</code></pre> <p>Optional configuration for the application.</p>"},{"location":"api/model_bundles/#launch.model_bundle.CloudpickleArtifactFlavor.framework","title":"framework  <code>class-attribute</code>","text":"<pre><code>framework: Union[\nPytorchFramework, TensorflowFramework, CustomFramework\n] = Field(Ellipsis, discriminator=\"framework_type\")\n</code></pre> <p>Machine Learning framework specification. Either <code>PytorchFramework</code>, <code>TensorflowFramework</code>, or <code>CustomFramework</code>.</p>"},{"location":"api/model_bundles/#launch.model_bundle.CloudpickleArtifactFlavor.load_model_fn","title":"load_model_fn  <code>class-attribute</code>","text":"<pre><code>load_model_fn: str\n</code></pre> <p>Function which, when called, returns the model object.</p>"},{"location":"api/model_bundles/#launch.model_bundle.CloudpickleArtifactFlavor.load_predict_fn","title":"load_predict_fn  <code>class-attribute</code>","text":"<pre><code>load_predict_fn: str\n</code></pre> <p>Function which, when called, returns the prediction function.</p>"},{"location":"api/model_bundles/#launch.model_bundle.CloudpickleArtifactFlavor.requirements","title":"requirements  <code>class-attribute</code>","text":"<pre><code>requirements: List[str]\n</code></pre> <p>List of requirements to install in the environment before running the model.</p>"},{"location":"api/model_bundles/#launch.model_bundle.CreateModelBundleV2Response","title":"CreateModelBundleV2Response","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response object for creating a Model Bundle.</p>"},{"location":"api/model_bundles/#launch.model_bundle.CreateModelBundleV2Response.model_bundle_id","title":"model_bundle_id  <code>class-attribute</code>","text":"<pre><code>model_bundle_id: str\n</code></pre> <p>ID of the Model Bundle.</p>"},{"location":"api/model_bundles/#launch.model_bundle.CustomFramework","title":"CustomFramework","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/model_bundles/#launch.model_bundle.CustomFramework.image_repository","title":"image_repository  <code>class-attribute</code>","text":"<pre><code>image_repository: str\n</code></pre> <p>Docker image repository to use as the base image.</p>"},{"location":"api/model_bundles/#launch.model_bundle.CustomFramework.image_tag","title":"image_tag  <code>class-attribute</code>","text":"<pre><code>image_tag: str\n</code></pre> <p>Docker image tag to use as the base image.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ListModelBundlesV2Response","title":"ListModelBundlesV2Response","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response object for listing Model Bundles.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ListModelBundlesV2Response.model_bundles","title":"model_bundles  <code>class-attribute</code>","text":"<pre><code>model_bundles: List[ModelBundleV2Response]\n</code></pre> <p>A list of Model Bundles.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundle","title":"ModelBundle  <code>dataclass</code>","text":"<p>Represents a ModelBundle.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundle.app_config","title":"app_config  <code>class-attribute</code>","text":"<pre><code>app_config: Optional[Dict[Any, Any]] = None\n</code></pre> <p>An optional user-specified configuration mapping for the bundle.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundle.env_params","title":"env_params  <code>class-attribute</code>","text":"<pre><code>env_params: Optional[Dict[str, str]] = None\n</code></pre> <p>A dictionary that dictates environment information. See LaunchClient.create_model_bundle for more information.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundle.id","title":"id  <code>class-attribute</code>","text":"<pre><code>id: Optional[str] = None\n</code></pre> <p>A globally unique identifier for the bundle.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundle.location","title":"location  <code>class-attribute</code>","text":"<pre><code>location: Optional[str] = None\n</code></pre> <p>An opaque location for the bundle.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundle.metadata","title":"metadata  <code>class-attribute</code>","text":"<pre><code>metadata: Optional[Dict[Any, Any]] = None\n</code></pre> <p>Arbitrary metadata for the bundle.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundle.name","title":"name  <code>class-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>The name of the bundle. Must be unique across all bundles that the user owns.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundle.packaging_type","title":"packaging_type  <code>class-attribute</code>","text":"<pre><code>packaging_type: Optional[str] = None\n</code></pre> <p>The packaging type for the bundle. Can be <code>cloudpickle</code> or <code>zip</code>.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundle.requirements","title":"requirements  <code>class-attribute</code>","text":"<pre><code>requirements: Optional[List[str]] = None\n</code></pre> <p>A list of Python package requirements for the bundle. See LaunchClient.create_model_bundle for more information.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundleV2Response","title":"ModelBundleV2Response","text":"<p>         Bases: <code>BaseModel</code></p> <p>Response object for a single Model Bundle.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundleV2Response.created_at","title":"created_at  <code>class-attribute</code>","text":"<pre><code>created_at: datetime.datetime\n</code></pre> <p>Timestamp of when the Model Bundle was created.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundleV2Response.flavor","title":"flavor  <code>class-attribute</code>","text":"<pre><code>flavor: Union[\nCloudpickleArtifactFlavor,\nZipArtifactFlavor,\nRunnableImageFlavor,\n] = Field(Ellipsis, discriminator=\"flavor\")\n</code></pre> <p>Flavor of the Model Bundle, representing how the model bundle was packaged. Either <code>CloudpickleArtifactFlavor</code>, <code>ZipArtifactFlavor</code>, or <code>RunnableImageFlavor</code>.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundleV2Response.id","title":"id  <code>class-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>ID of the Model Bundle.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundleV2Response.metadata","title":"metadata  <code>class-attribute</code>","text":"<pre><code>metadata: Dict[str, Any]\n</code></pre> <p>Metadata associated with the Model Bundle.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundleV2Response.model_artifact_ids","title":"model_artifact_ids  <code>class-attribute</code>","text":"<pre><code>model_artifact_ids: List[str]\n</code></pre> <p>IDs of the Model Artifacts associated with the Model Bundle.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ModelBundleV2Response.name","title":"name  <code>class-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Name of the Model Bundle.</p>"},{"location":"api/model_bundles/#launch.model_bundle.PytorchFramework","title":"PytorchFramework","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/model_bundles/#launch.model_bundle.PytorchFramework.pytorch_image_tag","title":"pytorch_image_tag  <code>class-attribute</code>","text":"<pre><code>pytorch_image_tag: str\n</code></pre> <p>Image tag of the Pytorch image to use.</p>"},{"location":"api/model_bundles/#launch.model_bundle.RunnableImageFlavor","title":"RunnableImageFlavor","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/model_bundles/#launch.model_bundle.RunnableImageFlavor.command","title":"command  <code>class-attribute</code>","text":"<pre><code>command: List[str]\n</code></pre> <p>Command to run the image.</p>"},{"location":"api/model_bundles/#launch.model_bundle.RunnableImageFlavor.env","title":"env  <code>class-attribute</code>","text":"<pre><code>env: Optional[Dict[str, str]]\n</code></pre> <p>Environment variables to set when running the image.</p>"},{"location":"api/model_bundles/#launch.model_bundle.RunnableImageFlavor.repository","title":"repository  <code>class-attribute</code>","text":"<pre><code>repository: str\n</code></pre> <p>Docker repository of the image.</p>"},{"location":"api/model_bundles/#launch.model_bundle.RunnableImageFlavor.tag","title":"tag  <code>class-attribute</code>","text":"<pre><code>tag: str\n</code></pre> <p>Docker tag of the image.</p>"},{"location":"api/model_bundles/#launch.model_bundle.TensorflowFramework","title":"TensorflowFramework","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/model_bundles/#launch.model_bundle.TensorflowFramework.tensorflow_version","title":"tensorflow_version  <code>class-attribute</code>","text":"<pre><code>tensorflow_version: str\n</code></pre> <p>Tensorflow version to use.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ZipArtifactFlavor","title":"ZipArtifactFlavor","text":"<p>         Bases: <code>BaseModel</code></p>"},{"location":"api/model_bundles/#launch.model_bundle.ZipArtifactFlavor.app_config","title":"app_config  <code>class-attribute</code>","text":"<pre><code>app_config: Optional[Dict[str, Any]]\n</code></pre> <p>Optional configuration for the application.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ZipArtifactFlavor.framework","title":"framework  <code>class-attribute</code>","text":"<pre><code>framework: Union[\nPytorchFramework, TensorflowFramework, CustomFramework\n] = Field(Ellipsis, discriminator=\"framework_type\")\n</code></pre> <p>Machine Learning framework specification. Either <code>PytorchFramework</code>, <code>TensorflowFramework</code>, or <code>CustomFramework</code>.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ZipArtifactFlavor.load_model_fn_module_path","title":"load_model_fn_module_path  <code>class-attribute</code>","text":"<pre><code>load_model_fn_module_path: str\n</code></pre> <p>Path to the module to load the model object.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ZipArtifactFlavor.load_predict_fn_module_path","title":"load_predict_fn_module_path  <code>class-attribute</code>","text":"<pre><code>load_predict_fn_module_path: str\n</code></pre> <p>Path to the module to load the prediction function.</p>"},{"location":"api/model_bundles/#launch.model_bundle.ZipArtifactFlavor.requirements","title":"requirements  <code>class-attribute</code>","text":"<pre><code>requirements: List[str]\n</code></pre> <p>List of requirements to install in the environment before running the model.</p>"},{"location":"api/model_endpoints/","title":"Model Endpoints","text":"<p>All classes here are returned by the  <code>get_model_endpoint</code> method and provide a <code>predict</code> function.</p>"},{"location":"api/model_endpoints/#launch.model_endpoint.AsyncEndpoint","title":"AsyncEndpoint","text":"<pre><code>AsyncEndpoint(model_endpoint: ModelEndpoint, client)\n</code></pre> <p>         Bases: <code>Endpoint</code></p> <p>An asynchronous model endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>model_endpoint</code> <code>ModelEndpoint</code> <p>ModelEndpoint object.</p> required <code>client</code> <p>A LaunchClient object</p> required"},{"location":"api/model_endpoints/#launch.model_endpoint.AsyncEndpoint.predict","title":"predict","text":"<pre><code>predict(request: EndpointRequest) -&gt; EndpointResponseFuture\n</code></pre> <p>Runs an asynchronous prediction request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>EndpointRequest</code> <p>The <code>EndpointRequest</code> object that contains the payload.</p> required <p>Returns:</p> Name Type Description <code>EndpointResponseFuture</code> <p>An <code>EndpointResponseFuture</code> such the user can use to query the status of the request.</p> <code>Example</code> <code>EndpointResponseFuture</code> <code>EndpointResponseFuture</code> <p>.. code-block:: python</p> <p>my_endpoint = AsyncEndpoint(...) f: EndpointResponseFuture = my_endpoint.predict(EndpointRequest(...)) result = f.get()  # blocks on completion</p>"},{"location":"api/model_endpoints/#launch.model_endpoint.AsyncEndpoint.predict_batch","title":"predict_batch","text":"<pre><code>predict_batch(\nrequests: Sequence[EndpointRequest],\n) -&gt; AsyncEndpointBatchResponse\n</code></pre> <p>(deprecated) Runs inference on the data items specified by urls. Returns a AsyncEndpointResponse.</p> <p>Parameters:</p> Name Type Description Default <code>requests</code> <code>Sequence[EndpointRequest]</code> <p>List of EndpointRequests. Request_ids must all be distinct.</p> required <p>Returns:</p> Type Description <code>AsyncEndpointBatchResponse</code> <p>an AsyncEndpointResponse keeping track of the inference requests made</p>"},{"location":"api/model_endpoints/#launch.model_endpoint.SyncEndpoint","title":"SyncEndpoint","text":"<pre><code>SyncEndpoint(model_endpoint: ModelEndpoint, client)\n</code></pre> <p>         Bases: <code>Endpoint</code></p> <p>A synchronous model endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>model_endpoint</code> <code>ModelEndpoint</code> <p>ModelEndpoint object.</p> required <code>client</code> <p>A LaunchClient object</p> required"},{"location":"api/model_endpoints/#launch.model_endpoint.SyncEndpoint.predict","title":"predict","text":"<pre><code>predict(request: EndpointRequest) -&gt; EndpointResponse\n</code></pre> <p>Runs a synchronous prediction request.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>EndpointRequest</code> <p>The <code>EndpointRequest</code> object that contains the payload.</p> required"},{"location":"concepts/batch_jobs/","title":"Batch Jobs","text":"<p>For predicting over a larger set of tasks (&gt; 50) at once, it is recommended to use batch jobs. Batch jobs are a way to send a large number of tasks to a model bundle. The tasks are processed in parallel, and the results are returned as a list of predictions.</p> <p>Batch jobs are created using the <code>batch_async_request</code> method of the <code>LaunchClient</code>.</p> Creating and Following a Batch Job<pre><code>import os\nimport time\nfrom launch import LaunchClient\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nbatch_job = client.batch_async_request(\nmodel_bundle=\"test-bundle\",\ninputs=[\n{\"x\": 2, \"y\": \"hello\"},\n{\"x\": 3, \"y\": \"world\"},\n],\nlabels={\n\"team\": \"MY_TEAM\",\n\"product\": \"MY_PRODUCT\",\n}\n)\nstatus = \"PENDING\"\nres = None\nwhile status != \"SUCCESS\" and status != \"FAILURE\" and status != \"CANCELLED\":\ntime.sleep(30)\nres = client.get_batch_async_response(batch_job[\"job_id\"])\nstatus = res[\"status\"]\nprint(f\"the batch job is {status}\")\nprint(res)\n</code></pre>"},{"location":"concepts/callbacks/","title":"Callbacks","text":"<p>Async model endpoints can be configured to send callbacks to a user-defined callback URL. Callbacks are sent as HTTP POST requests with a JSON body. The following code snippet shows how to create an async model endpoint with a callback URL.</p> <p>To configure an async endpoint to send callbacks, set the <code>post_inference_hooks</code> field to include <code>launch.PostInferenceHooks.CALLBACK</code>. A callback URL also needs to be specified, and it can be configured as a default using the <code>default_callback_url</code> argument to <code>launch.LaunchClient.create_model_endpoint</code> or as a per-task override using the <code>callback_url</code> field of <code>launch.EndpointRequest</code>.</p> <p>Note</p> <p>Callbacks will not be sent if the endpoint does not have any post-inference hooks specified, even if a <code>default_callback_url</code> is provided to the endpoint creation method or if the prediction request has a <code>callback_url</code> override.</p> Creating an Async Model Endpoint with a Callback URL<pre><code>import os\nimport time\nfrom launch import EndpointRequest, LaunchClient, PostInferenceHooks\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nendpoint = client.create_model_endpoint(\nendpoint_name=\"demo-endpoint-callback\",\nmodel_bundle=\"test-bundle\",\ncpus=1,\nmin_workers=1,\nendpoint_type=\"async\",\nupdate_if_exists=True,\nlabels={\n\"team\": \"MY_TEAM\",\n\"product\": \"MY_PRODUCT\",\n},\npost_inference_hooks=[PostInferenceHooks.CALLBACK],\ndefault_callback_url=\"https://example.com\",\n)\nwhile endpoint.status() != \"READY\":\ntime.sleep(10)\nfuture_default = endpoint.predict(\nrequest=EndpointRequest(args={\"x\": 2, \"y\": \"hello\"})\n)\n\"\"\"\nA callback is sent to https://example.com with the following JSON body:\n{\n    \"task_id\": \"THE_TASK_ID\",\n    \"result\": 7\n}\n\"\"\"\nfuture_custom_callback_url = endpoint.predict(\nrequest=EndpointRequest(\nargs={\"x\": 3, \"y\": \"hello\"}, callback_url=\"https://example.com/custom\"\n),\n)\n\"\"\"\nA callback is sent to https://example.com/custom with the following JSON body:\n{\n    \"task_id\": \"THE_TASK_ID\",\n    \"result\": 8\n}\n\"\"\"\n</code></pre>"},{"location":"concepts/callbacks/#authentication-for-callbacks","title":"Authentication for callbacks","text":"<p>Warning</p> <p>This feature is currently in beta, and the API is likely to change.</p> <p>Callbacks can be authenticated using shared authentication headers. To enable authentication, set either <code>default_callback_auth_kind</code> when creating the endpoint or <code>callback_auth_kind</code> when making a prediction request.</p> <p>Currently, the supported authentication methods are <code>basic</code> and <code>mtls</code>. If <code>basic</code> is used, then the <code>default_callback_auth_username</code> and <code>default_callback_auth_password</code> fields must be specified when creating the endpoint, or the <code>callback_auth_username</code> and <code>callback_auth_password</code> fields must be specified when making a prediction request. If <code>mtls</code> is used, then the  same is true for the <code>default_callback_auth_cert</code> and <code>default_callback_auth_key</code> fields, or the <code>callback_auth_cert</code> and <code>callback_auth_key</code> fields.</p> Creating an Async Model Endpoint with custom Callback auth<pre><code>import os\nimport time\nfrom launch import EndpointRequest, LaunchClient, PostInferenceHooks\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nendpoint = client.create_model_endpoint(\nendpoint_name=\"demo-endpoint-callback\",\nmodel_bundle=\"test-bundle\",\ncpus=1,\nmin_workers=1,\nendpoint_type=\"async\",\nupdate_if_exists=True,\nlabels={\n\"team\": \"MY_TEAM\",\n\"product\": \"MY_PRODUCT\",\n},\npost_inference_hooks=[PostInferenceHooks.CALLBACK],\ndefault_callback_url=\"https://example.com\",\ndefault_callback_auth_kind=\"basic\",\ndefault_callback_auth_username=\"user\",\ndefault_callback_auth_password=\"password\",\n)\nwhile endpoint.status() != \"READY\":\ntime.sleep(10)\nfuture_default = endpoint.predict(\nrequest=EndpointRequest(args={\"x\": 2, \"y\": \"hello\"})\n)\n\"\"\"\nA callback is sent to https://example.com with (\"user\", \"password\") as the basic auth.\n\"\"\"\nfuture_custom_callback_auth = endpoint.predict(\nrequest=EndpointRequest(\nargs={\"x\": 3, \"y\": \"hello\"},\ncallback_auth_kind=\"mtls\", \ncallback_auth_cert=\"cert\", \ncallback_auth_key=\"key\",\n),\n)\n\"\"\"\nA callback is sent with mTLS authentication.\n\"\"\"\nclient.edit_model_endpoint(\nmodel_endpoint=endpoint.model_endpoint,\ndefault_callback_auth_kind=\"mtls\",\ndefault_callback_auth_cert=\"cert\",\ndefault_callback_auth_key=\"key\",\n)\nwhile endpoint.status() != \"READY\":\ntime.sleep(10)\nfuture_default = endpoint.predict(\nrequest=EndpointRequest(args={\"x\": 2, \"y\": \"hello\"})\n)\n\"\"\"\nA callback is sent with mTLS auth.\n\"\"\"\nfuture_custom_callback_auth = endpoint.predict(\nrequest=EndpointRequest(\nargs={\"x\": 3, \"y\": \"hello\"},\ncallback_auth_kind=\"basic\",\ncallback_auth_username=\"user\",\ncallback_auth_password=\"pass\",\n),\n)\n\"\"\"\nA callback is sent with (\"user\", \"pass\") as the basic auth.\n\"\"\"\n</code></pre>"},{"location":"concepts/endpoint_predictions/","title":"Endpoint Predictions","text":"<p>Once endpoints have been created, users can send tasks to them to make predictions. The following code snippet shows how to send tasks to endpoints.</p> Sending a Task to an Async EndpointSending a Task to a Sync Endpoint <pre><code>import os\nfrom launch import EndpointRequest, LaunchClient\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nendpoint = client.get_model_endpoint(\"demo-endpoint-async\")\nfuture = endpoint.predict(request=EndpointRequest(args={\"x\": 2, \"y\": \"hello\"}))\nresponse = future.get()\nprint(response)\n</code></pre> <pre><code>import os\nfrom launch import EndpointRequest, LaunchClient\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nendpoint = client.get_model_endpoint(\"demo-endpoint-sync\")\nresponse = endpoint.predict(request=EndpointRequest(args={\"x\": 2, \"y\": \"hello\"}))\nprint(response)\n</code></pre>"},{"location":"concepts/endpoint_predictions/#launch.model_endpoint.EndpointRequest","title":"EndpointRequest","text":"<pre><code>EndpointRequest(\nurl: Optional[str] = None,\nargs: Optional[Dict] = None,\ncallback_url: Optional[str] = None,\ncallback_auth_kind: Optional[\nLiteral[basic, mtls]\n] = None,\ncallback_auth_username: Optional[str] = None,\ncallback_auth_password: Optional[str] = None,\ncallback_auth_cert: Optional[str] = None,\ncallback_auth_key: Optional[str] = None,\nreturn_pickled: Optional[bool] = False,\nrequest_id: Optional[str] = None,\n)\n</code></pre> <p>Represents a single request to either a <code>SyncEndpoint</code> or <code>AsyncEndpoint</code>.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Optional[str]</code> <p>A url to some file that can be read in to a ModelBundle's predict function. Can be an image, raw text, etc. Note: the contents of the file located at <code>url</code> are opened as a sequence of <code>bytes</code> and passed to the predict function. If you instead want to pass the url itself as an input to the predict function, see <code>args</code>.</p> <p>Exactly one of <code>url</code> and <code>args</code> must be specified.</p> <code>None</code> <code>args</code> <code>Optional[Dict]</code> <p>A Dictionary with arguments to a ModelBundle's predict function. If the predict function has signature <code>predict_fn(foo, bar)</code>, then the keys in the dictionary should be <code>\"foo\"</code> and <code>\"bar\"</code>. Values must be native Python objects.</p> <p>Exactly one of <code>url</code> and <code>args</code> must be specified.</p> <code>None</code> <code>return_pickled</code> <code>Optional[bool]</code> <p>Whether the output should be a pickled python object, or directly returned serialized json.</p> <code>False</code> <code>callback_url</code> <code>Optional[str]</code> <p>The callback url to use for this task. If None, then the default_callback_url of the endpoint is used. The endpoint must specify \"callback\" as a post-inference hook for the callback to be triggered.</p> <code>None</code> <code>callback_auth_kind</code> <code>Optional[Literal[basic, mtls]]</code> <p>The default callback auth kind to use for async endpoints. Either \"basic\" or \"mtls\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>callback_auth_username</code> <code>Optional[str]</code> <p>The default callback auth username to use. This only applies if callback_auth_kind is \"basic\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>callback_auth_password</code> <code>Optional[str]</code> <p>The default callback auth password to use. This only applies if callback_auth_kind is \"basic\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>callback_auth_cert</code> <code>Optional[str]</code> <p>The default callback auth cert to use. This only applies if callback_auth_kind is \"mtls\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>callback_auth_key</code> <code>Optional[str]</code> <p>The default callback auth key to use. This only applies if callback_auth_kind is \"mtls\". This can be overridden in the task parameters for each individual task.</p> <code>None</code> <code>request_id</code> <code>Optional[str]</code> <p>(deprecated) A user-specifiable id for requests. Should be unique among EndpointRequests made in the same batch call. If one isn't provided the client will generate its own.</p> <code>None</code>"},{"location":"concepts/endpoint_predictions/#launch.model_endpoint.EndpointResponseFuture","title":"EndpointResponseFuture","text":"<pre><code>EndpointResponseFuture(\nclient, endpoint_name: str, async_task_id: str\n)\n</code></pre> <p>Represents a future response from an Endpoint. Specifically, when the <code>EndpointResponseFuture</code> is ready, then its <code>get</code> method will return an actual instance of <code>EndpointResponse</code>.</p> <p>This object should not be directly instantiated by the user.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>An instance of <code>LaunchClient</code>.</p> required <code>endpoint_name</code> <code>str</code> <p>The name of the endpoint.</p> required <code>async_task_id</code> <code>str</code> <p>An async task id.</p> required"},{"location":"concepts/endpoint_predictions/#launch.model_endpoint.EndpointResponseFuture.get","title":"get","text":"<pre><code>get(timeout: Optional[float] = None) -&gt; EndpointResponse\n</code></pre> <p>Retrieves the <code>EndpointResponse</code> for the prediction request after it completes. This method blocks.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>Optional[float]</code> <p>The maximum number of seconds to wait for the response. If None, then the method will block indefinitely until the response is ready.</p> <code>None</code>"},{"location":"concepts/endpoint_predictions/#launch.model_endpoint.EndpointResponse","title":"EndpointResponse","text":"<pre><code>EndpointResponse(\nclient,\nstatus: str,\nresult_url: Optional[str] = None,\nresult: Optional[str] = None,\ntraceback: Optional[str] = None,\n)\n</code></pre> <p>Represents a response received from a Endpoint.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <p>An instance of <code>LaunchClient</code>.</p> required <code>status</code> <code>str</code> <p>A string representing the status of the request, i.e. <code>SUCCESS</code>, <code>FAILURE</code>, or <code>PENDING</code></p> required <code>result_url</code> <code>Optional[str]</code> <p>A string that is a url containing the pickled python object from the Endpoint's predict function.</p> <p>Exactly one of <code>result_url</code> or <code>result</code> will be populated, depending on the value of <code>return_pickled</code> in the request.</p> <code>None</code> <code>result</code> <code>Optional[str]</code> <p>A string that is the serialized return value (in json form) of the Endpoint's predict function. Specifically, one can <code>json.loads()</code> the value of result to get the original python object back.</p> <p>Exactly one of <code>result_url</code> or <code>result</code> will be populated, depending on the value of <code>return_pickled</code> in the request.</p> <code>None</code> <code>traceback</code> <code>Optional[str]</code> <p>The stack trace if the inference endpoint raised an error. Can be used for debugging</p> <code>None</code>"},{"location":"concepts/model_bundles/","title":"Model Bundles","text":"<p>Model Bundles are deployable models that can be used to make predictions. They are created by packaging a model up into a deployable format. </p>"},{"location":"concepts/model_bundles/#creating-model-bundles","title":"Creating Model Bundles","text":"<p>There are two methods for creating model bundles: <code>create_model_bundle</code> and <code>create_model_bundle_from_dirs</code>. The former directly pickles a user-specified <code>load_predict_fn</code>, a function which loads the model and returns a <code>predict_fn</code>, a function which takes in a request. The latter takes in directories containing a <code>load_predict_fn</code> and the module path to the <code>load_predict_fn</code>.</p> Creating a Model Bundle DirectlyCreating a Model Bundle from Directories <pre><code>import os\nfrom pydantic import BaseModel\nfrom launch import LaunchClient\nclass MyRequestSchema(BaseModel):\nx: int\ny: str\nclass MyResponseSchema(BaseModel):\n__root__: int\ndef my_load_predict_fn(model):\ndef returns_model_of_x_plus_len_of_y(x: int, y: str) -&gt; int:\n\"\"\"MyRequestSchema -&gt; MyResponseSchema\"\"\"\nassert isinstance(x, int) and isinstance(y, str)\nreturn model(x) + len(y)\nreturn returns_model_of_x_plus_len_of_y\ndef my_model(x):\nreturn x * 2\nENV_PARAMS = {\n\"framework_type\": \"pytorch\",\n\"pytorch_image_tag\": \"1.7.1-cuda11.0-cudnn8-runtime\",\n}\nBUNDLE_PARAMS = {\n\"model_bundle_name\": \"test-bundle\",\n\"model\": my_model,\n\"load_predict_fn\": my_load_predict_fn,\n\"env_params\": ENV_PARAMS,\n\"requirements\": [\"pytest==7.2.1\", \"numpy\"],  # list your requirements here\n\"request_schema\": MyRequestSchema,\n\"response_schema\": MyResponseSchema,\n}\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nclient.create_model_bundle(**BUNDLE_PARAMS)\n</code></pre> <pre><code>import os\nimport tempfile\nfrom pydantic import BaseModel\nfrom launch import LaunchClient\ndirectory = tempfile.mkdtemp()\nmodel_filename = os.path.join(directory, \"model.py\")\nwith open(model_filename, \"w\") as f:\nf.write(\"\"\"\ndef my_load_model_fn():\n    def my_model(x):\n        return x * 2\n    return my_model\n\"\"\")\npredict_filename = os.path.join(directory, \"predict.py\")\nwith open(predict_filename, \"w\") as f:\nf.write(\"\"\"\ndef my_load_predict_fn(model):\n    def returns_model_of_x_plus_len_of_y(x: int, y: str) -&gt; int:\n        assert isinstance(x, int) and isinstance(y, str)\n        return model(x) + len(y)\n    return returns_model_of_x_plus_len_of_y\n\"\"\")\nrequirements_filename = os.path.join(directory, \"requirements.txt\")\nwith open(predict_filename, \"w\") as f:\nf.write(\"\"\"\npytest==7.2.1\nnumpy\n\"\"\"\n)\n\"\"\"\nThe directory structure should now look like\ndirectory/\n    model.py\n    predict.py\n    requirements.txt\n\"\"\"\nclass MyRequestSchema(BaseModel):\nx: int\ny: str\nclass MyResponseSchema(BaseModel):\n__root__: int\nENV_PARAMS = {\n\"framework_type\": \"pytorch\",\n\"pytorch_image_tag\": \"1.7.1-cuda11.0-cudnn8-runtime\",\n}\nBUNDLE_PARAMS = {\n\"model_bundle_name\": \"test-bundle\",\n\"base_paths\": [directory],\n\"requirements_path\": \"requirements.txt\",\n\"env_params\": ENV_PARAMS,\n\"load_predict_fn\": \"predict.my_load_predict_fn\",\n\"load_model_fn_module_path\": \"model.my_load_model_fn\",\n\"request_schema\": MyRequestSchema,\n\"response_schema\": MyResponseSchema,\n}\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nclient.create_model_bundle_from_dirs(**BUNDLE_PARAMS)\n# Clean up files from demo\nos.remove(model_filename)\nos.remove(predict_filename)\nos.rmdir(directory)\n</code></pre>"},{"location":"concepts/model_bundles/#configuring-model-bundles","title":"Configuring Model Bundles","text":"<p>The <code>app_config</code> field of a model bundle is a dictionary that can be used to configure the model bundle. If specified, the <code>app_config</code> is passed to the <code>load_predict_fn</code> when the model bundle is deployed, alongside the <code>model</code>. This can allow for more code reuse between multiple bundles that perform similar tasks.</p> Creating Model Bundles with app_config<pre><code>import os\nfrom launch import LaunchClient\nfrom pydantic import BaseModel\nfrom typing import List, Union\nfrom typing_extensions import Literal\nclass MyRequestSchemaSingle(BaseModel):\nkind: Literal['single']\nx: int\ny: str\nclass MyRequestSchemaBatched(BaseModel):\nkind: Literal['batched']\nx: List[int]\ny: List[str]\nclass MyRequestSchema(BaseModel):\n__root__: Union[MyRequestSchemaSingle, MyRequestSchemaBatched]\nclass MyResponseSchema(BaseModel):\n__root__: Union[int, List[int]]\ndef my_load_predict_fn(app_config, model):\ndef returns_model_of_x_plus_len_of_y(x: Union[int, List[int]], y: Union[str, List[str]]) -&gt; Union[int, List[int]]:\n\"\"\"MyRequestSchema -&gt; MyResponseSchema\"\"\"\nif app_config[\"mode\"] == \"single\":\nassert isinstance(x, int) and isinstance(y, str)\nreturn model(x) + len(y)\nresult = []\nfor x_i, y_i in zip(x, y):\nresult.append(model(x_i) + len(y_i))\nreturn result\nreturn returns_model_of_x_plus_len_of_y\ndef my_load_model_fn(app_config):\ndef my_model_single(x: int):\nreturn x * 2\ndef my_model_batched(x: List[int]):\nreturn [my_model_single(x_i) for x_i in x]\nif app_config[\"mode\"] == \"single\":\nreturn my_model_single\nreturn my_model_batched\nENV_PARAMS = {\n\"framework_type\": \"pytorch\",\n\"pytorch_image_tag\": \"1.7.1-cuda11.0-cudnn8-runtime\",\n}\nBUNDLE_PARAMS_SINGLE = {\n\"model_bundle_name\": \"test-bundle-single\",\n\"load_predict_fn\": my_load_predict_fn,\n\"load_model_fn\": my_load_model_fn,\n\"env_params\": ENV_PARAMS,\n\"requirements\": [\"pytest==7.2.1\", \"numpy\"],\n\"request_schema\": MyRequestSchema,\n\"response_schema\": MyResponseSchema,\n\"app_config\": {\"mode\": \"single\"},\n}\nBUNDLE_PARAMS_BATCHED = {\n\"model_bundle_name\": \"test-bundle-batched\",\n\"load_predict_fn\": my_load_predict_fn,\n\"load_model_fn\": my_load_model_fn,\n\"env_params\": ENV_PARAMS,\n\"requirements\": [\"pytest==7.2.1\", \"numpy\"],\n\"request_schema\": MyRequestSchema,\n\"response_schema\": MyResponseSchema,\n\"app_config\": {\"mode\": \"batched\"},\n}\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nbundle_single = client.create_model_bundle(**BUNDLE_PARAMS_SINGLE)\nbundle_batch = client.create_model_bundle(**BUNDLE_PARAMS_BATCHED)\n</code></pre>"},{"location":"concepts/model_bundles/#updating-model-bundles","title":"Updating Model Bundles","text":"<p>Model Bundles are immutable, meaning they cannot be edited once created. However, it is possible to clone an existing model bundle with a new <code>app_config</code> using <code>clone_model_bundle_with_changes</code>.</p>"},{"location":"concepts/model_bundles/#listing-model-bundles","title":"Listing Model Bundles","text":"<p>To list all the model bundles you own, use <code>list_model_bundles</code>.</p>"},{"location":"concepts/model_endpoints/","title":"Model Endpoints","text":"<p>Model Endpoints are deployments of models that can receive requests and return predictions containing the results of the model's inference. Each model endpoint is associated with a model bundle, which contains the model's code. An endpoint specifies deployment parameters, such as the minimum and maximum number of workers, as well as the requested resources for each worker, such as the number of CPUs, amount of memory, GPU count, and type of GPU.</p> <p>Endpoints can be asynchronous or synchronous. Asynchronous endpoints return a future immediately after receiving a request, and the future can be used to retrieve the prediction once it is ready. Synchronous endpoints return the prediction directly after receiving a request.</p> <p>Info</p>"},{"location":"concepts/model_endpoints/#choosing-the-right-inference-mode","title":"Choosing the right inference mode","text":"<p>Here are some tips for how to choose between SyncEndpoint, AsyncEndpoint, and BatchJob for deploying your ModelBundle:</p> <p>A SyncEndpoint is good if:</p> <ul> <li>You have strict latency requirements (e.g. on the order of seconds or less).</li> <li>You are willing to have resources continually allocated.</li> </ul> <p>An AsyncEndpoint is good if:</p> <ul> <li>You want to save on compute costs.</li> <li>Your inference code takes a long time to run.</li> <li>Your latency requirements are on the order of minutes.</li> </ul> <p>A BatchJob is good if:</p> <ul> <li>You know there is a large batch of inputs ahead of time.</li> <li>You want to optimize for throughput instead of latency.</li> </ul>"},{"location":"concepts/model_endpoints/#creating-async-model-endpoints","title":"Creating Async Model Endpoints","text":"<p>Async model endpoints are the most cost-efficient way to perform inference on tasks that are less latency-sensitive.</p> Creating an Async Model Endpoint<pre><code>import os\nfrom launch import LaunchClient\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nendpoint = client.create_model_endpoint(\nendpoint_name=\"demo-endpoint-async\",\nmodel_bundle=\"test-bundle\",\ncpus=1,\nmin_workers=0,\nendpoint_type=\"async\",\nupdate_if_exists=True,\nlabels={\n\"team\": \"MY_TEAM\",\n\"product\": \"MY_PRODUCT\",\n},\n)\n</code></pre>"},{"location":"concepts/model_endpoints/#creating-sync-model-endpoints","title":"Creating Sync Model Endpoints","text":"<p>Sync model endpoints are useful for latency-sensitive tasks, such as real-time inference. Sync endpoints are more expensive than async endpoints.</p> <p>Note</p> <p>Sync model endpoints require at least 1 <code>min_worker</code>.</p> Creating a Sync Model Endpoint<pre><code>import os\nfrom launch import LaunchClient\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nendpoint = client.create_model_endpoint(\nendpoint_name=\"demo-endpoint-sync\",\nmodel_bundle=\"test-bundle\",\ncpus=1,\nmin_workers=1,\nendpoint_type=\"sync\",\nupdate_if_exists=True,\nlabels={\n\"team\": \"MY_TEAM\",\n\"product\": \"MY_PRODUCT\",\n},\n)\n</code></pre>"},{"location":"concepts/model_endpoints/#managing-model-endpoints","title":"Managing Model Endpoints","text":"<p>Model endpoints can be listed, updated, and deleted using the Launch API.</p> Listing Model Endpoints<pre><code>import os\nfrom launch import LaunchClient\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nendpoints = client.list_model_endpoints()\n</code></pre> Updating a Model Endpoint<pre><code>import os\nfrom launch import LaunchClient\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nclient.edit_model_endpoint(\nmodel_endpoint=\"demo-endpoint\",\nmax_workers=2,\n)\n</code></pre> Deleting a Model Endpoint<pre><code>import os\nfrom launch import LaunchClient\nclient = LaunchClient(api_key=os.getenv(\"LAUNCH_API_KEY\"))\nendpoint = client.create_model_endpoint(\nendpoint_name=\"demo-endpoint-tmp\",\nmodel_bundle=\"test-bundle\",\ncpus=1,\nmin_workers=0,\nendpoint_type=\"async\",\nupdate_if_exists=True,\nlabels={\n\"team\": \"MY_TEAM\",\n\"product\": \"MY_PRODUCT\",\n},\n)\nclient.delete_model_endpoint(model_endpoint=\"demo-endpoint-tmp\")\n</code></pre>"},{"location":"concepts/overview/","title":"Overview","text":"<p>Creating deployments on Launch generally involves three steps:</p> <ol> <li> <p>Create and upload a <code>ModelBundle</code>. Pass your trained model    as well as pre-/post-processing code to the Scale Launch Python client, and     we\u2019ll create a model bundle based on the code and store it in our Bundle Store.</p> </li> <li> <p>Create a <code>ModelEndpoint</code>. Pass a ModelBundle as well as    infrastructure settings such as the desired number of GPUs to our client.    This provisions resources on Scale\u2019s cluster dedicated to your ModelEndpoint.</p> </li> <li> <p>Make requests to the ModelEndpoint. You can make requests through the Python    client, or make HTTP requests directly to Scale.</p> </li> </ol>"}]}