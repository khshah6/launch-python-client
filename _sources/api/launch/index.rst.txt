:py:mod:`launch`
================

.. py:module:: launch

.. autoapi-nested-parse::

   Scale Launch provides ML engineers with a simple Python interface for turning a local code snippet into a
   production service that automatically scales according to traffic.




Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   cli/index.rst


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   request_validation/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   launch.AsyncEndpoint
   launch.AsyncEndpointBatchResponse
   launch.Connection
   launch.EndpointRequest
   launch.EndpointResponse
   launch.EndpointResponseFuture
   launch.LaunchClient
   launch.ModelBundle
   launch.SyncEndpoint




.. py:class:: AsyncEndpoint(model_endpoint, client)



   An asynchronous model endpoint.

   :param model_endpoint: ModelEndpoint object.
   :param client: A LaunchClient object

   .. py:method:: predict(self, request)

      Runs an asynchronous prediction request.

      :param request: The ``EndpointRequest`` object that contains the payload.

      :returns: An ``EndpointResponseFuture`` such the user can use to query the status of the request.
                Example:

                .. code-block:: python

                   my_endpoint = AsyncEndpoint(...)
                   f: EndpointResponseFuture = my_endpoint.predict(EndpointRequest(...))
                   result = f.get()  # blocks on completion


   .. py:method:: predict_batch(self, requests)

      (deprecated)
      Runs inference on the data items specified by urls. Returns a AsyncEndpointResponse.

      :param requests: List of EndpointRequests. Request_ids must all be distinct.

      :returns: an AsyncEndpointResponse keeping track of the inference requests made


   .. py:method:: resource_settings(self)

      Gets the resource settings of the Endpoint.


   .. py:method:: status(self)

      Gets the status of the Endpoint.


   .. py:method:: worker_settings(self)

      Gets the worker settings of the Endpoint.



.. py:class:: AsyncEndpointBatchResponse(client, endpoint_name, request_ids)

   (deprecated)

   Currently represents a list of async inference requests to a specific endpoint. Keeps track of the requests made,
   and gives a way to poll for their status.

   Invariant: set keys for self.request_ids and self.responses are equal

   idk about this abstraction tbh, could use a redesign maybe?

   Also batch inference sort of removes the need for much of the complication in here


   .. py:method:: get_responses(self)

      Returns a dictionary, where each key is the request_id for an EndpointRequest passed in, and the corresponding
      object at that key is the corresponding EndpointResponse.


   .. py:method:: is_done(self, poll=True)

      Checks the client local state to see if all requests are done.

      :param poll: If ``True``, then this will first check the state for a subset
      :param of the remaining incomplete tasks on the Launch server.:


   .. py:method:: poll_endpoints(self)

      Runs one round of polling the endpoint for async task results.



.. py:class:: Connection(api_key, endpoint = None)

   Wrapper of HTTP requests to the Launch endpoint.

   .. py:method:: make_request(self, payload, route, requests_command=requests.post)

      Makes a request to Launch endpoint and logs a warning if not
      successful.

      :param payload: given payload
      :param route: route for the request
      :param requests_command: requests.post, requests.get, requests.delete
      :return: response JSON



.. py:class:: EndpointRequest(url = None, args = None, return_pickled = True, request_id = None)

   Represents a single request to either a ``SyncEndpoint`` or ``AsyncEndpoint``.

   :param url: A url to some file that can be read in to a ModelBundle's predict function. Can be an image, raw text, etc.
               **Note**: the contents of the file located at ``url`` are opened as a sequence of ``bytes`` and passed
               to the predict function. If you instead want to pass the url itself as an input to the predict function,
               see ``args``.

               Exactly one of ``url`` and ``args`` must be specified.
   :param args: A Dictionary with arguments to a ModelBundle's predict function. If the predict function has signature
                ``predict_fn(foo, bar)``, then the keys in the dictionary should be ``"foo"`` and ``"bar"``.
                Values must be native Python objects.

                Exactly one of ``url`` and ``args`` must be specified.
   :param return_pickled: Whether the output should be a pickled python object, or directly returned serialized json.
   :param request_id: (deprecated) A user-specifiable id for requests.
                      Should be unique among EndpointRequests made in the same batch call.
                      If one isn't provided the client will generate its own.


.. py:class:: EndpointResponse(client, status, result_url = None, result = None, traceback = None)

   Represents a response received from a Endpoint.


   :param client: An instance of ``LaunchClient``.
   :param status: A string representing the status of the request, i.e. ``SUCCESS``, ``FAILURE``, or ``PENDING``
   :param result_url: A string that is a url containing the pickled python object from the Endpoint's predict function.

                      Exactly one of ``result_url`` or ``result`` will be populated,
                      depending on the value of ``return_pickled`` in the request.
   :param result: A string that is the serialized return value (in json form) of the Endpoint's predict function.
                  Specifically, one can ``json.loads()`` the value of result to get the original python object back.

                  Exactly one of ``result_url`` or ``result`` will be populated,
                  depending on the value of ``return_pickled`` in the request.
   :param traceback: The stack trace if the inference endpoint raised an error. Can be used for debugging


.. py:class:: EndpointResponseFuture(client, endpoint_name, async_task_id)

   Represents a future response from an Endpoint. Specifically, when the ``EndpointResponseFuture`` is ready,
   then its ``get`` method will return an actual instance of ``EndpointResponse``.

   This object should not be directly instantiated by the user.

   :param client: An instance of ``LaunchClient``.
   :param endpoint_name: The name of the endpoint.
   :param async_task_id: An async task id.

   .. py:method:: get(self)

      Retrieves the ``EndpointResponse`` for the prediction request after it completes. This method blocks.



.. py:class:: LaunchClient(api_key, endpoint = None, self_hosted = False)

   Scale Launch Python Client.

   Initializes a Scale Launch Client.

   :param api_key: Your Scale API key
   :param endpoint: The Scale Launch Endpoint (this should not need to be changed)
   :param self_hosted: True iff you are connecting to a self-hosted Scale Launch

   .. py:method:: async_request(self, endpoint_name, url = None, args = None, return_pickled = True)

      Not recommended to use this, instead we recommend to use functions provided by AsyncEndpoint.
      Makes a request to the Async Model Endpoint at endpoint_id, and immediately returns a key that can be used to retrieve
      the result of inference at a later time.

      :param endpoint_name: The name of the endpoint to make the request to
      :param url: A url that points to a file containing model input.
                  Must be accessible by Scale Launch, hence it needs to either be public or a signedURL.
                  **Note**: the contents of the file located at ``url`` are opened as a sequence of ``bytes`` and passed
                  to the predict function. If you instead want to pass the url itself as an input to the predict function,
                  see ``args``.

                  Exactly one of ``url`` and ``args`` must be specified.
      :param args: A dictionary of arguments to the ModelBundle's predict function.
                   Must be json-serializable, i.e. composed of ``str``, ``int``, ``float``, etc.
                   If your predict function has signature ``predict(foo, bar)``, then args should be a dictionary with
                   keys ``"foo"`` and ``"bar"``.

                   Exactly one of ``url`` and ``args`` must be specified.
      :param return_pickled: Whether the python object returned is pickled, or directly written to the file returned.

      :returns: An id/key that can be used to fetch inference results at a later time.
                Example output:
                    `abcabcab-cabc-abca-0123456789ab`


   .. py:method:: batch_async_request(self, model_bundle, urls = None, inputs = None, batch_url_file_location = None, serialization_format = 'json', batch_task_options = None, labels = None)

      Sends a batch inference request using a given bundle. Returns a key that can be used to retrieve
      the results of inference at a later time.

      Must have exactly one of urls or inputs passed in.

      :param model_bundle: The bundle or the name of a the bundle to use for inference.
      :param urls: A list of urls, each pointing to a file containing model input.
                   Must be accessible by Scale Launch, hence urls need to either be public or signedURLs.
      :param inputs: A list of model inputs, if exists, we will upload the inputs and pass it in to Launch.
      :param batch_url_file_location: In self-hosted mode, the input to the batch job will be uploaded
                                      to this location if provided. Otherwise, one will be determined from bundle_location_fn()
      :param serialization_format: Serialization format of output, either 'pickle' or 'json'.
                                   'pickle' corresponds to pickling results + returning
      :param batch_task_options: A Dict of optional endpoint/batch task settings, i.e. certain endpoint settings
                                 like ``cpus``, ``memory``, ``gpus``, ``gpu_type``, ``max_workers``, as well as under-the-hood batch
                                 job settings, like ``pyspark_partition_size``, ``pyspark_max_executors``.

      :returns: An id/key that can be used to fetch inference results at a later time


   .. py:method:: create_model_bundle(self, model_bundle_name, env_params, *, load_predict_fn = None, predict_fn_or_cls = None, requirements = None, model = None, load_model_fn = None, bundle_url = None, app_config = None, globals_copy = None)

      Uploads and registers a model bundle to Scale Launch.

      A model bundle consists of exactly one of the following:

      - ``predict_fn_or_cls``
      - ``load_predict_fn + model``
      - ``load_predict_fn + load_model_fn``

      Pre/post-processing code can be included inside load_predict_fn/model or in predict_fn_or_cls call.

      :param model_bundle_name: The name of the model bundle you want to create. The name must be unique across all
                                bundles that you own.
      :param predict_fn_or_cls: ``Function`` or a ``Callable`` class that runs end-to-end (pre/post processing and model inference) on the call.
                                i.e. ``predict_fn_or_cls(REQUEST) -> RESPONSE``.
      :param model: Typically a trained Neural Network, e.g. a Pytorch module.

                    Exactly one of ``model`` and ``load_model_fn`` must be provided.
      :param load_model_fn: A function that, when run, loads a model. This function is essentially a deferred
                            wrapper around the ``model`` argument.

                            Exactly one of ``model`` and ``load_model_fn`` must be provided.
      :param load_predict_fn: Function that, when called with a model, returns a function that carries out inference.

                              If ``model`` is specified, then this is equivalent
                              to:
                                  ``load_predict_fn(model, app_config=optional_app_config]) -> predict_fn``

                              Otherwise, if ``load_model_fn`` is specified, then this is equivalent
                              to:
                                  ``load_predict_fn(load_model_fn(), app_config=optional_app_config]) -> predict_fn``

                              In both cases, ``predict_fn`` is then the inference function, i.e.:
                                  ``predict_fn(REQUEST) -> RESPONSE``
      :param requirements: A list of python package requirements, where each list element is of the form
                           ``<package_name>==<package_version>``, e.g.

                           ``["tensorflow==2.3.0", "tensorflow-hub==0.11.0"]``

                           If you do not pass in a value for ``requirements``, then you must pass in ``globals()`` for the
                           ``globals_copy`` argument.
      :param app_config: Either a Dictionary that represents a YAML file contents or a local path to a YAML file.
      :param env_params: A dictionary that dictates environment information e.g.
                         the use of pytorch or tensorflow, which base image tag to use, etc.
                         Specifically, the dictionary should contain the following keys:

                         - ``framework_type``: either ``tensorflow`` or ``pytorch``.
                         - PyTorch fields:
                             - ``pytorch_image_tag``: An image tag for the ``pytorch`` docker base image. The list of tags
                                 can be found from https://hub.docker.com/r/pytorch/pytorch/tags.
                             - Example:

                             .. code-block:: python

                                {
                                    "framework_type": "pytorch",
                                    "pytorch_image_tag": "1.10.0-cuda11.3-cudnn8-runtime"
                                }

                         - Tensorflow fields:
                             - ``tensorflow_version``: Version of tensorflow, e.g. ``"2.3.0"``.
      :param globals_copy: Dictionary of the global symbol table. Normally provided by ``globals()`` built-in function.
      :param bundle_url: (Only used in self-hosted mode.) The desired location of bundle.
                         Overrides any value given by ``self.bundle_location_fn``


   .. py:method:: create_model_bundle_from_dirs(self, model_bundle_name, base_paths, requirements_path, env_params, load_predict_fn_module_path, load_model_fn_module_path, app_config = None)

      Packages up code from one or more local filesystem folders and uploads them as a bundle to Scale Launch.
      In this mode, a bundle is just local code instead of a serialized object.

      For example, if you have a directory structure like so, and your current working directory is also ``my_root``:

      .. code-block:: text

         my_root/
             my_module1/
                 __init__.py
                 ...files and directories
                 my_inference_file.py
             my_module2/
                 __init__.py
                 ...files and directories

      then calling ``create_model_bundle_from_dirs`` with ``base_paths=["my_module1", "my_module2"]`` essentially
      creates a zip file without the root directory, e.g.:

      .. code-block:: text

         my_module1/
             __init__.py
             ...files and directories
             my_inference_file.py
         my_module2/
             __init__.py
             ...files and directories

      and these contents will be unzipped relative to the server side application root. Bear these points in mind when
      referencing Python module paths for this bundle. For instance, if ``my_inference_file.py`` has ``def f(...)``
      as the desired inference loading function, then the `load_predict_fn_module_path` argument should be
      `my_module1.my_inference_file.f`.


      :param model_bundle_name: The name of the model bundle you want to create. The name must be unique across all
                                bundles that you own.
      :param base_paths: The paths on the local filesystem where the bundle code lives.
      :param requirements_path: A path on the local filesystem where a ``requirements.txt`` file lives.
      :param env_params: A dictionary that dictates environment information e.g.
                         the use of pytorch or tensorflow, which base image tag to use, etc.
                         Specifically, the dictionary should contain the following keys:

                         - ``framework_type``: either ``tensorflow`` or ``pytorch``.
                         - PyTorch fields:
                             - ``pytorch_image_tag``: An image tag for the ``pytorch`` docker base image. The list of tags
                                 can be found from https://hub.docker.com/r/pytorch/pytorch/tags.
                             - Example:

                             .. code-block:: python

                                {
                                    "framework_type": "pytorch",
                                    "pytorch_image_tag": "1.10.0-cuda11.3-cudnn8-runtime"
                                }
      :param load_predict_fn_module_path: A python module path for a function that, when called with the output of
                                          load_model_fn_module_path, returns a function that carries out inference.
      :param load_model_fn_module_path: A python module path for a function that returns a model. The output feeds into
                                        the function located at load_predict_fn_module_path.
      :param app_config: Either a Dictionary that represents a YAML file contents or a local path to a YAML file.


   .. py:method:: create_model_endpoint(self, endpoint_name, model_bundle, cpus = 3, memory = '8Gi', gpus = 0, min_workers = 1, max_workers = 1, per_worker = 1, gpu_type = None, endpoint_type = 'sync', post_inference_hooks = None, update_if_exists = False, labels = None)

      Creates and registers a model endpoint in Scale Launch. The returned object is an instance of type ``Endpoint``,
      which is a base class of either ``SyncEndpoint`` or ``AsyncEndpoint``. This is the object
      to which you sent inference requests.

      :param endpoint_name: The name of the model endpoint you want to create. The name must be unique across
                            all endpoints that you own.
      :param model_bundle: The ``ModelBundle`` that the endpoint should serve.
      :param cpus: Number of cpus each worker should get, e.g. 1, 2, etc. This must be greater than or equal to 1.
      :param memory: Amount of memory each worker should get, e.g. "4Gi", "512Mi", etc. This must be a positive
                     amount of memory.
      :param gpus: Number of gpus each worker should get, e.g. 0, 1, etc.
      :param min_workers: The minimum number of workers. Must be greater than or equal to 0.
      :param max_workers: The maximum number of workers. Must be greater than or equal to 0, and as well as
                          greater than or equal to ``min_workers``.
      :param per_worker: The maximum number of concurrent requests that an individual worker can service. Launch
                         automatically scales the number of workers for the endpoint so that each worker is processing
                         ``per_worker`` requests:

                         - If the average number of concurrent requests per worker is lower than ``per_worker``, then the number
                           of workers will be reduced.
                         - Otherwise, if the average number of concurrent requests per worker is higher
                           than ``per_worker``, then the number of workers will be increased to meet the elevated traffic.
      :param gpu_type: If specifying a non-zero number of gpus, this controls the type of gpu requested. Here are the
                       supported values:

                       - ``nvidia-tesla-t4``
                       - ``nvidia-ampere-a10``
      :param endpoint_type: Either ``"sync"`` or ``"async"``.
      :param post_inference_hooks: List of hooks to trigger after inference tasks are served.
      :param update_if_exists: If ``True``, will attempt to update the endpoint if it exists. Otherwise, will
                               unconditionally try to create a new endpoint. Note that endpoint names for a given user must be unique,
                               so attempting to call this function with ``update_if_exists=False`` for an existing endpoint will raise
                               an error.
      :param labels: An optional dictionary of key/value pairs to associate with this endpoint.

      :returns: A Endpoint object that can be used to make requests to the endpoint.


   .. py:method:: delete_model_bundle(self, model_bundle)

      Deletes the model bundle.

      :param model_bundle: A ``ModelBundle`` object or the name of a model bundle.


   .. py:method:: delete_model_endpoint(self, model_endpoint)

      Deletes a model endpoint.

      :param model_endpoint: A ``ModelEndpoint`` object.


   .. py:method:: edit_model_endpoint(self, model_endpoint, model_bundle = None, cpus = None, memory = None, gpus = None, min_workers = None, max_workers = None, per_worker = None, gpu_type = None, post_inference_hooks = None)

      Edits an existing model endpoint. Here are the fields that **cannot** be edited on an existing endpoint:

      - The endpoint's name.
      - The endpoint's type (i.e. you cannot go from a ``SyncEnpdoint`` to an ``AsyncEndpoint`` or vice versa.

      :param model_endpoint: The model endpoint (or its name) you want to edit. The name must be unique across
                             all endpoints that you own.
      :param model_bundle: The ``ModelBundle`` that the endpoint should serve.
      :param cpus: Number of cpus each worker should get, e.g. 1, 2, etc. This must be greater than or equal to 1.
      :param memory: Amount of memory each worker should get, e.g. "4Gi", "512Mi", etc. This must be a positive
                     amount of memory.
      :param gpus: Number of gpus each worker should get, e.g. 0, 1, etc.
      :param min_workers: The minimum number of workers. Must be greater than or equal to 0.
      :param max_workers: The maximum number of workers. Must be greater than or equal to 0, and as well as
                          greater than or equal to ``min_workers``.
      :param per_worker: The maximum number of concurrent requests that an individual worker can service. Launch
                         automatically scales the number of workers for the endpoint so that each worker is processing
                         ``per_worker`` requests:

                         - If the average number of concurrent requests per worker is lower than ``per_worker``, then the number
                           of workers will be reduced.
                         - Otherwise, if the average number of concurrent requests per worker is higher
                           than ``per_worker``, then the number of workers will be increased to meet the elevated traffic.
      :param gpu_type: If specifying a non-zero number of gpus, this controls the type of gpu requested. Here are the
                       supported values:

                       - ``nvidia-tesla-t4``
                       - ``nvidia-ampere-a10``
      :param endpoint_type: Either ``"sync"`` or ``"async"``.
      :param post_inference_hooks: List of hooks to trigger after inference tasks are served.


   .. py:method:: get_async_response(self, async_task_id)

      Not recommended to use this, instead we recommend to use functions provided by ``AsyncEndpoint``.
      Gets inference results from a previously created task.

      :param async_task_id: The id/key returned from a previous invocation of ``async_request``.

      :returns: A dictionary that contains task status and optionally a result url or result if the task has completed.
                Result url or result will be returned if the task has succeeded. Will return a result url iff
                ``return_pickled`` was set to ``True`` on task creation.

                The dictionary's keys are as follows:

                - ``state``: ``'PENDING'`` or ``'SUCCESS'`` or ``'FAILURE'``
                - ``result_url``: a url pointing to inference results. This url is accessible for 12 hours after the request has been made.
                - ``result``: the value returned by the endpoint's `predict` function, serialized as json

                Example output:

                .. code-block:: json

                    {
                        'state': 'SUCCESS',
                        'result_url': 'https://foo.s3.us-west-2.amazonaws.com/bar/baz/qux?xyzzy'
                    }


   .. py:method:: get_batch_async_response(self, batch_async_task_id)

      Gets inference results from a previously created batch job.

      :param batch_async_task_id: An id representing the batch task job. This id is the in the response from
                                  calling ``batch_async_request``.

      :returns:

                - ``status``: The status of the job.
                - ``result``: The url where the result is stored.
                - ``duration``: A string representation of how long the job took to finish.
      :rtype: A dictionary that contains the following fields


   .. py:method:: get_model_bundle(self, model_bundle)

      Returns a model bundle specified by ``bundle_name`` that the user owns.

      :param model_bundle: The bundle or its name.

      :returns: A ``ModelBundle`` object


   .. py:method:: get_model_endpoint(self, endpoint_name)

      Gets a model endpoint associated with a name.

      :param endpoint_name: The name of the endpoint to retrieve.


   .. py:method:: list_model_bundles(self)

      Returns a list of model bundles that the user owns.

      :returns: A list of ModelBundle objects


   .. py:method:: list_model_endpoints(self)

      Lists all model endpoints that the user owns.

      :returns: A list of ``ModelEndpoint`` objects.


   .. py:method:: read_endpoint_creation_logs(self, model_endpoint)

      Retrieves the logs for the creation of the endpoint.

      :param model_endpoint: The endpoint or its name.


   .. py:method:: register_batch_csv_location_fn(self, batch_csv_location_fn)

      For self-hosted mode only. Registers a function that gives a location for batch CSV inputs. Should give different
      locations each time. This function is called as batch_csv_location_fn(), and should return a batch_csv_url that
      upload_batch_csv_fn can take.

      Strictly, batch_csv_location_fn() does not need to return a str. The only requirement is that if batch_csv_location_fn
      returns a value of type T, then upload_batch_csv_fn() takes in an object of type T as its second argument
      (i.e. batch_csv_url).

      :param batch_csv_location_fn: Function that generates batch_csv_urls for upload_batch_csv_fn.


   .. py:method:: register_bundle_location_fn(self, bundle_location_fn)

      For self-hosted mode only. Registers a function that gives a location for a model bundle. Should give different
      locations each time. This function is called as ``bundle_location_fn()``, and should return a ``bundle_url``
      that ``register_upload_bundle_fn`` can take.

      Strictly, ``bundle_location_fn()`` does not need to return a ``str``. The only requirement is that if
      ``bundle_location_fn`` returns a value of type ``T``, then ``upload_bundle_fn()`` takes in an object of type T
      as its second argument (i.e. bundle_url).

      :param bundle_location_fn: Function that generates bundle_urls for upload_bundle_fn.


   .. py:method:: register_endpoint_auth_decorator(self, endpoint_auth_decorator_fn)

      For self-hosted mode only. Registers a function that modifies the endpoint creation payload to include
      required fields for self-hosting.


   .. py:method:: register_upload_batch_csv_fn(self, upload_batch_csv_fn)

      For self-hosted mode only. Registers a function that handles batch text upload. This function is called as

          upload_batch_csv_fn(csv_text, csv_url)

      This function should directly write the contents of ``csv_text`` as a text string into ``csv_url``.

      :param upload_batch_csv_fn: Function that takes in a csv text (string type), and uploads that bundle to an appropriate
                                  location. Only needed for self-hosted mode.


   .. py:method:: register_upload_bundle_fn(self, upload_bundle_fn)

      For self-hosted mode only. Registers a function that handles model bundle upload. This function is called as

          upload_bundle_fn(serialized_bundle, bundle_url)

      This function should directly write the contents of ``serialized_bundle`` as a
      binary string into ``bundle_url``.

      See ``register_bundle_location_fn`` for more notes on the signature of ``upload_bundle_fn``

      :param upload_bundle_fn: Function that takes in a serialized bundle (bytes type), and uploads that bundle to an appropriate
                               location. Only needed for self-hosted mode.



.. py:class:: ModelBundle

   Represents a ModelBundle.

   .. py:attribute:: bundle_id
      :annotation: :Optional[str]

      A globally unique identifier for the bundle. This is not to be used in the API.

   .. py:attribute:: env_params
      :annotation: :Optional[Dict[str, str]]

      A dictionary that dictates environment information. See LaunchClient.create_model_bundle
      for more information.

   .. py:attribute:: location
      :annotation: :Optional[str]

      An opaque location for the bundle.

   .. py:attribute:: metadata
      :annotation: :Optional[Dict[Any, Any]]

      Arbitrary metadata for the bundle.

   .. py:attribute:: name
      :annotation: :str

      The name of the bundle. Must be unique across all bundles that the user owns.

   .. py:attribute:: packaging_type
      :annotation: :Optional[str]

      The packaging type for the bundle. Can be ``cloudpickle`` or ``zip``.

   .. py:attribute:: requirements
      :annotation: :Optional[List[str]]

      A list of Python package requirements for the bundle. See LaunchClient.create_model_bundle
      for more information.


.. py:class:: SyncEndpoint(model_endpoint, client)



   A synchronous model endpoint.

   :param model_endpoint: ModelEndpoint object.
   :param client: A LaunchClient object

   .. py:method:: predict(self, request)

      Runs a synchronous prediction request.

      :param request: The ``EndpointRequest`` object that contains the payload.


   .. py:method:: resource_settings(self)

      Gets the resource settings of the Endpoint.


   .. py:method:: status(self)

      Gets the status of the Endpoint.


   .. py:method:: worker_settings(self)

      Gets the worker settings of the Endpoint.



